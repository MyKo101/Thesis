[
["index.html", "Multi-State Clinical Prediction Models in Renal Replacement Therapy Chapter 1 Page One", " Multi-State Clinical Prediction Models in Renal Replacement Therapy Michael Andrew Barrowman March 2020 Chapter 1 Page One HTML Output is TRUE and LaTeX output is TRUE "],
["chap-lit-report.html", "Chapter 2 Literature Report 2.1 Introduction 2.2 Clinical Prediction Models 2.3 Competing Risks &amp; Multi-State Models", " Chapter 2 Literature Report 2.1 Introduction lorem ipsum blah blah blah 2.2 Clinical Prediction Models The idea of prognosis dates back to ancient Greece with the work of Hippocrates [1] and is derived from the Greek for “know before” meaning to forecast the future. Within the sphere of healthcare, it is definde as the risk of future health outcomes in patients, particularly patients with a certain disease or health condition. Prognosis allows clinicians to provide patients with a prediction of how their disease will progress and is uaully given as a probability of having an event in a prespecified number of years. For example, QRISK3 [2] provides a probability that a patient will have a heart attack or stroke in the next 10 years. Prognostic research encompasses any work which enhances the field of prognosis, whether through methodological advancements, field-specific prognostic modelling or educational material designed to improve general knowledge of prognosis. Prognostic models come under the wider umbrella of predictive models which also includes diagnostic models; because of this most of the keys points in the field or prognostic modeling can be applied to diagnostic models with little to no change. Prognosis allows clinicians to evaluate the natural history of a patient (i.e. the course of a patient’s future without any intervention) in order to establish the ffect of screening for asymptomatic diseases (such as with mammograms[3]). Prognosis research can be used to develop new definitions of diseases, whether a redefinition of an existing disease (such as the extension to th definition of myocardial infarction to include non-fatal events [4]) or a previously unknown subtype of a disease (such as Brugada syndrome as a type of cardiovascular disease[5]) In general, prognosis research can be broken down into four main categories, with three subcategories [6]: Type I: Fundamental prognosis research [3] Type II: Prognostic factor research [7] Type III: Prognostic model research [8] Model development [9] Model validation [10] Model impact evaluation [11] Type IV: Stratified Medicine [12] For a particular outcome, prognostic research will usually progress through these types, beginning with papers designed to evaluate overall prognosis within a whole population and then focusing in on more specificity and granularity towards individualised, causal predictions. The model development and validation will usually occur in the same paper [Cite: LR - 37, 38]. studies into all three of the subcategories of prognostic model research should be completed before a model is used in clinical practice [Cite: LR - 39], althouhg this does not always occur [Cite: LR - 3]. External validation is considered by some to be more important than the actual deviation of the model as it demonstrates generalisability of the model [Cite: LR - 12], whereas a model on it’s own may be highly susceptible to overfitting [Cite: Something]. 2.2.1 Fundamental Prognosis Research [What is it? Old definition is incorrect, so will need to write this fresh] 2.2.2 Prognostic Factor Research The aim of prognostic factor research (Type II) is to discover which factors are associated with disease progression. This allows for the general attribution of relationships between predictors and clinical outcomes. Predictive factor research can give researchers and clinicians an idea of which patient factors are important when assessing a disease. It is vital to the development of clinical predictive models as without an idea of what covariates can affect an outcome, we cannot figure out which variables will affect the outcome. For example, [xxxx] demonstrated that [xxxx] is correlated with [xxxx], which subsequently used as a covariate in the development of the [xxxx] model. Note the use of the word correlate here as prognostic relationships do not have to be causal ones [Cite: Something]. These factors may indeed represent an underlying causal pathway, but this is not a requirement and it would require aetiological methods to discern whether it were causal or not. For example, when predicting [xxxx], we can demonstrate that [xxxx] is a prognostic factor, [however since the arrow of causation is [xxxx]] [OR] [however since [xxxx] causes both [xxxx] and [xxxx]], the relationship is prognostic, but not causal. [Previously used Apgar score here, reference 40] Counter to the idea that prognostic factors aren’t always causal, they are always confounding factors for the event they predict. Thue prognostic factors should be taken into account when planning clinical trials as if they are wildly misbalanced across the arms (or not accounted for in some other manner), they can cause biases in the results [Cite: LR - 2]. Sometimes these factors are so strong that adjusting the results of a clinical trial by the factor can affect, or even reverse the interpretation of the results [Cite: LR - 44]. If a prognostic factor is causal, then by directly affecting the factor, it can causally affect the outcome. By discovering new prognostic factors, and investigating their causality, we can potentially open the door to new directions of attack for treatments. It is unfortunate, however, that Riley at al [Cite: LR - 53] found that only 35.5% of prognostic factor studies in paediatric oncology actually reported the size of the effect of the prognostic factor they reported on. This means that very little information can be drawn from these studies. It is also important that prognostic factor research papers consider and report on the implications of the factor they assess such as healthcare costs. These kinds of implications are rarely assessed, especially when compared to drugs or interventions [Cite: LR - 2]. 2.2.3 Prognostic Model Research Predictive factors can be combined into a predictive model, which is a much more specific measurement of the effect of a factor on an outcome [Cite: LR - 3] and they are deigned to augment the job of a clinician; and not to completely replace them [Cite: LR - 40]. Diagnostic prediction model can be used to indicate whether a patient is likely to need further testing to establish the presence of a disease [Cite: LR - 37, 38]. Prognostic prediction models can be used to decide on further treatment for that patient, whether as a member of a certain risk group, or under a stratied medicine approach [Cite: LR - 37, 38]. Outcomes being assessed in a prediction model should be directly relevant to the patient (such as mortality) or have a direct causal relationship with something that is [Cite: LR - 40]. There is a trend of researchers focusing on areas of improvement that are of less significance to the patient than it is to a physician [Cite: LR - 18]. For example, older patient’s might prefer to have an improved quality of life than an increase in life expectancy, and thus models should be developed to account for this. Creating a clinicaly useful model is not as simple as just using some availble data to develop a model, despite what a lot of researchers seem to believe [Cite: Something]. To quote Steyerberg et al [Cite: LR - 3]. \" To be useful for clinicia,s a prognostic model needs to provide validated and accurate predictions and to improve patient outcomes and cost-effectiveness of care\". This means that, although a mdel might appear to be useful, its effectiveness is only relevant to the population it was developed in. If your population is different, then the model will behave differently. Bleeker [Cite: LR - 54] developed a model to predict bacterial infections in febrile children with an unknown source. The model scored well when assessed for the predictive value in the development dataset, however it scored much worse in an external dataset implying that, though it worked well in the development population, it would be unwise to apply it to a new population. 2.2.3.1 Model Development The first stage of having a useful model is to develop one. Clinical predictive models can take a variety of forms, such as logistic regression, cox models or some kind of machine learning. Regardless of the specific model type being used, there are certain universal truths than should be held up during model development which will be discussed here. The size of the dataset being used is of vital importance as it can combat overfitting of the data, but so is choosing which prognostic factors to be included in the final model. This section will discuss various ideas that researchers need to account for when developing a model from any source and can be applied to any model type. By considering a multivariable approach to prediction models (as opposed to a univariable one), researchers can consider different combinations of predictive factors, usually refered to as potential predictors [Cite: LR - 2]. These can include factors where a direct relationship with the disease can be clearly seen, such as tumour size in the prediction of cancer mortality [Cite: LR - 5], or ones which could have a more general effect on overall health, such as socioeconomic and ethnicity variables [Cite: LR - 55]. By ignoring any previous assumptions about a correlation between these potential predictors and the outcome of interest, we can cast a wider net in our analysis allowing us to catch relationships that might have otherwise been lost [Cite: LR - 56]. Prediction models should take into account as many predictive factors as possible. Demographic data should also be included as these are often found to be confounding factors, variables such as ethnicity and social deprivation risk exacerbating the existing inequality between groups r LR(7)`. When developing a predictive model, the size of the dataset being used in an important consideration. A typical “rule of thumb” is to have at least 10 events for every potential predictor [Cite: LR - 57, 58], know as the Events-per-Variable (EPV). Recently, this number has been superseded by a methods to evaluate a specific required sample size [Cite: Riley, EPV work]. If there aren’t enough events to satisfy this criteria, then some potential predictors should be eliminated before any formal analysis takes place (for example using clinical knowledge) [Cite: LR - 59]. In general, it is also recommended that this development dataset contain at least 100 events (regardless of number of potential predictors) [Cite: LR - 39, 60, 61]. A systematic review by Counsell et al [Cite: LR - 62] found that out of eighty-three prognostic models for acute stroke, less than 50% of them had more than 10 EPV, and the work by Riley et al [Cite: riley EPV Work] showed that less that [Pull example from Riley EPV]. Having a low EPV can lead to overfitting of the model which is a concern associated with having a small data set. Overfitting leads to a worse prediction when the model is used on a new population which essentially makes the model useless[Cite: LR - 34]. However, just because a dataset is large does not imply that it will be a good dataset if the quality of the data is lacking [Cite: LR - 39]. Having a large amount of data can lead to predictors being considered statistically significant when in reality they only add a small amount of information to the model [Cite: LR - 39]. The size of the effect of a predictor should therefore be taken into account in the final model and, if beneficial, some predictors can be dropped at the final stage. Large datasets can be used for both development and validation if an effective subset is chosen. This subset should not be random or data driven and should be decided before data analysis is begun [Cite: LR - 39]. Randomly splitting a dataset set into a training set (for development) and a testing set (for internal validation) can result in optimistic results in the validation process in the testing set. This is due to the random nature of the splitting causing the two populations to be too exchangeable, which is similar to the logic behind the splitting of patients in a Randomised Control Trial (RCT). Splitting the population by a specific characteristic (such as geographic location or time period) can result in a better internal validation [Cite: LR - 35, 63]. Derivation of the QRISK2 Score [Cite: LR - 7] (known later as QRISK2-2008) randomly assigned two thirds of practices to the derivation dataset and the remainder to the validation dataset [Check how QRISK3 Does this]. The NPI model [Has this been mentioned?] was trained on the first 500 patients admitted to Nottingham City Hospital after the study began [Cite: LR - 5] and later validated on the next 320 patients to be admitted [Cite: LR - 64], this validation was not performed at the same time as the initial development and is thus an external validation. If a sufficient amount of data is available and it has been taken from multiple sources (practices, clinics or studies), then it should be clustered to account for heterogeneity across sources [Cite: LR - 65]. It is important that any sources of potential variability are identified (such as heterogeneity between centres) as this can have an impact on the results of any analysis [Cite: LR - 1, 39]. Heterogeneity is particularly high when using multiple countries as a source of data [Cite: LR - 66] or if a potential predictor is of a subjective nature, which leads to discrepancies between assessors [Cite: LR - 67]. Overlooking of this clustering can lead to incorrect inferences [Cite: LR - 65]. The generalisability of the sources of data should also be considered in the development of a model. For example, the inclusion and exclusion criteria of an RCT can greatly reduce generalisability if used as a data source [Cite: LR - 40]. 2.2.3.2 Model Validation 2.2.3.3 Impact Evaluation 2.2.4 Stratified Medicine 2.2.5 Examples 2.3 Competing Risks &amp; Multi-State Models "],
["chap-scoping-review.html", "Chapter 3 The Application of Multi-State Methods to Develop Clinical Prediction Models Designed for Clinical Use - A Scoping Review 3.1 Introduction 3.2 Methods", " Chapter 3 The Application of Multi-State Methods to Develop Clinical Prediction Models Designed for Clinical Use - A Scoping Review 3.1 Introduction eHealthcare is moving towards a more data-driven approach to decision making, exploiting the variety of data sources collected as part of routine care [1]. This increases efficiency, which is becoming increasingly vital as patients are living longer and requiring more care, while budgets are being reduced [2], [3]. Correspondingly, there has been a shift towards primary prevention, rather than purely treating disease as it arises [4] therefore clinical prediction models (CPMs) are more relevant than ever before [5]. Prognostic CPMs (those that predict the future) allow end-users to estimate an individual’s probability/risk of experiencing an outcome of interest within a certain timeframe. CPMs are algorithms that relate a set of prognostic factors to the risk of a chosen outcome [6], often using multivariable regression. They can provide predictions of the future course of an illness and provide evidence for the commencement of medical interventions [7]. Along with this overall increase in importance, different methods of producing CPMs are also being used, and each makes different assumptions, and models at different levels of granularity. One of these methods is the Multi-State Model (MSM), an extension to traditional survival analysis wherein patients exist in one of many distinct states at any given time and can transition between them (these individual transitions are akin to that of traditional survival analysis) [8]. A subset of MSMs is that of a Competing Risks model, where patients can only move from a single initial state to many absorbing states without any intermediate or transient states. A huge advantage of Multi-State CPMs, and indeed, Competing Risks CPMs, is that they can provide predictions for multiple outcomes with MSMs going further by allowing the prediction of multiple pathways to that outcome, whereas traditionally developed models only provide predictions for a single end-point. However, little is known about how widely these types of models are implemented in clinically relevant prognostic research. Therefore, we here aim to document a scoping review protocol that will intend to uncover any prediction models using MSMs that have been developed for clinical use. As part of the process of this investigation, we will also document how many CPMs account for Competing Risks alone. We define a scoping review as described by Arksey and O’Malley [9] , which is similar to a systematic review, but with less formal outline for the analysis and synthesis of literature [10]. By assessing how MSMs have currently been applied in this field, we aim to describe the landscape of their current use, the context in which they are being used and discuss ways in which their use, application and uptake can be improved. To the best of our knowledge, a review such as this for Multi-State Models has never been performed. 3.2 Methods \\[ A= \\pi r^2 \\] 3.2.1 Scope of Review This review will cover articles related to the development of Multi-State Clinical Prediction models designed for clinical use. It will not include models that were developed solely for demonstrations of novel methodological improvements in the field of clinical prediction modelling and/or multi-state modelling. Article inclusion will be based on the screening of the article text and interpretation of its aims, primary distinction will be made on whether an existing dataset is used as a core part of the article or as a subsidiary example. It will include articles that validate previously developed models and those that review existing models, only so far as to use them to find the original development article (a method known as Snowballing). As this analysis will follow the style of a scoping review; the final paper will adhere to the PRISMA-ScR guidelines [11], which were set out to extend the traditional PRISMA guidelines to a Scoping Review setting. Models which focus only on a competing risks scenario (whether directly or simply adjusting for competing risks) will not be analysed in detail, however to avoid missing possible Multi-State Models, we will only omit these at the final stage of screening (See below). This will also allow for a brief description of how many CR models exist compared to the MSM models to be analysed in detail in this review. As per the definitions set out by the PROGRESS research group, prognostic research is split into four overarching themes/types: Type I - Fundamental Prognosis Research [12] Type II - Prognostic Factor Research [13] Type III - Prognostic Model Research [14] Type IV - Stratified Medicine Research [15] As such, we will be focusing on papers of Type III [14]. Articles related to the other types of prognostic research often develop a model within their work, but since the intent of these papers is to investigate overall outcomes, effects of an individual factor or interactive effects of treatments in individuals, they are considered disjoint from CPM development and so they will not be included in our analysis. 3.2.2 Initial Search Strategy 3.2.2.1 Search Terms To ensure we cover as much of the medical literature as possible, we will use the Ovid search engine to search two databases: EMBASE (1974 to 2018 December 31) Ovid MEDLINE and Epub Ahead of Print, In-Process &amp; Other Non-Indexed Citations, Daily and Versions 1946 to December 31, 2018 We will use a standard set of terms designed by Ingui &amp; Rogers [16] and added to by Geersing et al [17] used for searching for clinical prediction related literature. We will also extend this by including search terms relating to time-to-event outcomes and/or survival analysis that were defined by the authors, and which aim to broaden our search (see table 1). This will be combined by a set of search terms designed to filter for MSMs and/or CRs. These novel MSM/CR terms include “fine adj2 gray” to include papers which use the Fine &amp; Gray subdistribution proportional hazard method [18]. It will also include“semimarkov or semi markov” to include articles which specify that the model adopts a semi-Markov perspective, which is common amongst MSMs [8]. However, we chose not to include the term “markov” alone as it is considered to be too unspecific to be of use (a la search for “model” alone when finding clinical prediction models). The full search details can be found in table 2. We believe that the broadness of our search terms allows for high sensitivity in our results and will therefore provide a larger and more comprehensive pool of papers than using a more specific set of search terms. [Insert Table from paper] 3.2.2.2 Validation set of articles To ensure that our search strategy is satisfactory, we will compare our results to a set of Validation papers. These are papers that we are already aware of that satisfy our inclusion/exclusion criteria and which therefore should be included in our analysis. We will compare the results of our initial search with this set of papers to ensure that all of the Validation set appear in our results. If they do not, then we will adjust our search strategy iteratively increasing sensitivity and improving the reach of our search until all Validation papers are included. The set of Validation papers is as follows: Estimation and Prediction in a Multi-State Model for Breast Cancer, Putter et al, 2006 [20] A Multi-State Model to Predict Heart Failure Hospitalizations and All-Cause Mortality in Outpatients With Heart Failure With Reduced Ejection Fraction: Model Derivation and External Validation, Upshaw et al, 2016 [21] Predicting timing of clinical outcomes in patients with chronic kidney disease and severely decreased glomerular filtration rate, Grams et al, 2018 [22] Estimating transition probability of different states of type 2 diabetes and its associated factors using Markov model, Nazari et al, 2018 [23] Advantages of a multi-state approach in surgical research: how intermediate events and risk factor profile affect the prognosis of a patient with locally advanced rectal cancer, Manzini et al, 2018 [24] 3.2.3 Filtering Once the initial set of articles has been found, these will be filtered at various degrees of granularity to focus on papers which are included in the scope of our review as per our inclusion/exclusion criteria. We will also define which papers will be used only for the snowballing process, but will not be used as part of our analysis. 3.2.3.1 Inclusion/Exclusion Criteria Inclusion Type III Prognostic Study Papers (i.e. those developing a clinical prediction model) [14] Papers which use a Multi-State Model framework to provide individual level patient predictions Exclusion Papers that develop overall population level predictions (Type I) Papers focused on identification of prognostic factors (Type II) Papers that investigate stratified medicine (Type IV) Papers that only develop Competing Risks models Papers designed to describe methodological models with or without clinical application used only for an example 3.2.3.2 Stages The filtering of the results will be performed in three stages: 1. Title (MB) 2. Abstract (MB with 20% replication by DJ) 3. Full Paper (MB with 20% replication by DJ) Filtering will begin with an initial check through all titles to assess whether it is believed that the paper may be relevant to the review. This will help to omit a large amount of papers that were incorrectly returned by the broad search strategy. To ensure the review remains as sensitive as possible, only papers where it is abundantly clear that they violate an inclusion/exclusion criteria will be removed at this stage. A second filter will be performed on the abstracts of the remaining articles and removed papers will be classified by the reason for their omission. To allow for faster data extraction, a final glancing filter will also be performed over the full papers to again reduce the numbers of collated papers in the final review and reduce the likelihood of removing papers at the analysis stage. To ensure robustness of this filtering, both of these stages will be replicated by a second reviewer (DJ) in a randomly selected 20% of the abstracts and papers and differences will be discussed internally. At this point, models focusing solely on competing risks (i.e. those without a transient state) will be filtered out. 3.2.4 Data Extraction To study the use of Multi-State Clinical Prediction Models from a quantitative perspective, certain vital data points will be extracted from the extant models. These measurements can be grouped as to what element of the prediction model they are evaluating: * Clinically Relevant points * Number of patients * Clinical setting (i.e. primary vs secondary care, geographic setting) * Field of study (e.g. cardiovascular, renal, etc.) * Summary of patient demographics (i.e. inclusion/exclusion criteria) * Outcomes being predicted * Multi-State Model details * Number of States and what they are * Shape/Structure of the model (i.e. how patients can transition between states) * How were relevant variables chosen? * Transition assumptions (e.g. parametric vs non-parametric, PH assumption, etc…) * Stated justification for, and reported benefits of an MSM versus traditional methods. * Predictive Ability * Timeframe (e.g. single time point(s), continuous time prediction, dynamic prediction, etc…) * What validation was performed (None vs. Internal (bootstrap, CV, etc.) vs. External) * Comparisons to current guidelines * Assessment of Bias of their model (using PROBAST) * Utilisation of the TRIPOD Guidelines (e.g. Was it referenced? Was it adhered to?) * Prominence information * Number of citations (although not clinically relevant, it is relevant to understanding the model’s utilisation) * Year of publication (again, not clinically relevant, but useful to spot any time trends in prominence and/or quality) The data extracted at this stage will be checked by DJ in 20% of the papers to confirm results for the analysis 3.2.5 Reporting The search and filtering strategy will be depicted with a modified PRISMA flow diagram [30], which includes papers found by Snowballing and how they are included in the filtration process, see figure 1. [Add in PRISMA] A table of the extracted information will be included with the paper, depending on the number of results, this may be supplementary material. This information will also be summarised and analysed both quantitatively and qualitatively. For example, as the Illness-Death model [8] is simple and common amongst multi-state models, we will count how many of the MSCPMs use this structure as well as the other most common structures used. Any direct comparisons that can be made between predictions of this type (i.e. from the same field with the same outcomes) will be described. "],
["chap-Conf-CR.html", "Chapter 4 How unmeasured confounding in a competing risks setting can affect treatment effect estimates in observational studies 4.1 Background 4.2 Methods", " Chapter 4 How unmeasured confounding in a competing risks setting can affect treatment effect estimates in observational studies 4.1 Background Well-designed observation studies permit researchers to assess treatment effects when randomisation is not feasible. This may be due to cost, suspected non-equipoise treatments or any number of other reasons [1]. While observational studies minimise these issues by being cheaper to run and avoiding randomisation (which, although unknown at the time, may prescribe patients to worse treatments), they are potentially subject to issues such as unmeasured confounding and increased possibility of competing risks (where multiple clinically relevant events occur). Although these issues can arise in any study, Randomised Controlled Trials (RCTs) attempt to mitigate these effects by using randomisation of treatment and strict inclusion/exclusion criteria. However, the estimated treatment effects from RCTs are of potentially limited generalisability, accessibility and implementability [2]. A confounder is a variable that is a common cause of both treatment and outcome. For example, a patient with a high Body Mass Index (BMI) is more likely to be prescribed statins [3], but are also more likely to suffer a cardiovascular event. These treatment decisions can be affected by variables that are not routinely collected (such as childhood socio-economic status or the severity of a comorbidity [4]. Therefore, if these variables are omitted form (or unavailable for) the analysis of treatment effects in observational studies, then they can bias inferences [5]. As well as having a direct effect on the event-of-interest, confounders (along with other covariates) can also have further reaching effects on a patient’s health by changing the chances of having a competing event. Patients who are more likely to have a competing event are less likely to have an event-of-interest, which can affect inferences from studies ignoring the competing event. In the above BMI example, a high BMI can also increase a patient’s likelihood of developing (and thus dying from) cancer [6]. The issue of confounding in observational studies has been researched previously [7,8,9], where it has been consistently shown that unmeasured confounding is likely to occur within these natural datasets and that there is poor reporting of this, even after the introduction of the The Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) Guidelines [10, 11]. Hence, it is widely recognised that sensitivity analyses are vital within the observational setting [12]. However these previous studies do not extend this work into a competing risk setting, meaning research in this space is lacking [13], particularly where the presence of a competing event can affect the rate of occurrence of the event-of-interest. These issues will commonly occur in elderly and comorbid patients where treatment decisions are more complex. As the elderly population grows, the clinical community needs to understand the optimal way to treat patients with complex conditions; here, causal relationships between treatment and outcome need to account for competing events appropriately. The most common way of analysing data that contains competing events is using a cause specific perspective, as in the Cox methodology [14], where competing events are considered as censoring events and analysis focuses solely on the event-of-interest. The alternative is to assume a subdistributional perspective, as in the Fine &amp; Gray methodology [15], where patients who have competing events remain in the risk set forever. The aim of this paper is to study the bias induced by the presence of unmeasured confounding on treatment effect estimates in the competing risks framework. We investigated how unmeasured confounding affects the apparent effect of treatment under the Fine &amp; Gray and the Cox methodologies and how these estimates differ from their true value. To accomplish this, we used simulations to generate synthetic time-to-event-data and then model under both perspectives. Both the Cox and Fine &amp; Gray models provide hazard ratios to describe the effects of a covariate. A binary covariate will represent a treatment and the coefficients found by the model will be the estimate of interest. 4.2 Methods We considered a simulation scenario in which our population can experience two events; one of which is the event-of-interest (Event 1), the other is a competing event (Event 2). We model a single unmeasured confounding covariate, \\(U \\sim N (0,1)\\) and a binary treatment indicator, \\(Z\\). We varied how much \\(U\\) and \\(Z\\) affect the probability distribution of the two events as well as how they are correlated. For example, \\(Z\\) could represent whether a patient is prescribed statins, U could be their BMI, the event-of-interest could be cardiovascular disease related mortality and a competing event could be cancer-related mortality. We followed best practice for conducting and reporting simulations studies [16]. The data-generating mechanism defined two cause-specific hazard functions (one for each event), where the baseline hazard for event 1 was \\(k\\) times that of event 2, see Fig. 1. We assumed a baseline hazard that was either constant (exponential distributed failure times), linearly increasing (Weibull distributed failure times) or biologically plausible [17]. The hazards used were thus: \\[\\begin{align} \\lambda_1(t|U,Z) &amp;= ke^{\\beta_1U + \\gamma_1Z}\\lambda_0(t)\\\\ \\lambda_2(t|U,Z) &amp;= ke^{\\beta_2U + \\gamma_2Z}\\lambda_0(t) \\end{align}\\] \\[\\begin{equation} \\lambda_0(t) \\begin{cases} 1 &amp; \\textrm{Exponential}\\\\ 2t &amp; \\textrm{Webull}\\\\ \\exp{-18+7.3t-11.5t^{0.5}\\log(t) + 9.5t^{0.5}} &amp; \\textrm{Plausible} \\end{cases} \\end{equation}\\] In the above equations, \\(\\beta\\) and \\(\\gamma\\) are the effects of the confounding covariate and the treatment effect respectively with the subscripts representing which event they are affecting. These two hazard functions entirely describe how a population will behave [18]. [Insert Figure 1] We simulated populations of 10,000 patients to ensure small confidence intervals around our treatment effect estimates in each simulation. Each simulated population had a distinct value for \\(\\beta\\) and \\(\\gamma\\). In order to simulate the confounding of \\(U\\) and \\(Z\\), we generated these values such that \\(\\textrm{Corr}(U,Z) = \\rho\\) and \\(\\Pr(Z = 1) = \\pi\\) [19]. Population end times and type of event were generated using the relevant hazard functions. The full process for the simulations can be found in Additional file 1. Due to the methods used to generate the populations, the possible values for \\(\\rho\\) are bounded by the choice of \\(\\pi\\) such that when \\(\\pi = 0.5\\), \\(\\left|\\rho\\right| &lt;= 0.797\\) and when \\(\\pi = 0.1\\) (or \\(\\pi=0.9\\)), \\(\\left|\\rho\\right| &lt;= 0.57\\). The relationship between the parameters can be seen in the Directed Acyclic Graph (DAG) shown in Fig. 2, where \\(T\\) is the event time and \\(\\delta\\) is the event type indicator (1 for event-of-interest and 2 for competing event). "],
["chap-IPCW-logistic.html", "Chapter 5 Inverse Probability Weighting Adjustment of the Logistic Regression Calibration-in-the-Large 5.1 Introduction 5.2 Methods 5.3 Results 5.4 Discussion 5.5 References 5.6 Supplementary Material - Calibration Slope", " Chapter 5 Inverse Probability Weighting Adjustment of the Logistic Regression Calibration-in-the-Large 5.1 Introduction Clinical prediction models (CPMs) need to be validated before they are used. A fundamental test of their validity is calibration: the agreement between observed and predicted outcomes. This requires that among individuals with p% risk of an event, p% of those have the event [13]. The simplest assessment of calibration is the calibration-in-the-large, which tests for agreement in mean calibration (the weakest form of calibration) [14]. With continuous or binary outcomes, such a test is straight-forward: it can be translated to a test for a zero intercept in a regression model with an appropriately transformed linear predictor as an offset, and no other predictors. In the case of Cox regression, however, estimation of calibration is complicated in three ways. First, calibration can be computed at multiple time-points and one must decide which time-points to evaluate, and how to integrate over these time-points. Second, there exists no explicit intercept in the model because of the non-parametric baseline hazard function [15]. Third, censoring needs to be handled in an appropriate way. The choice and combination of time-points determines what we mean by calibration; this is problem-specific and not the focus of this paper. Calibration can also be looked at integrated over time using martingale residuals[16]; however here we focus on the case where calibration at a specific time point is of interest - e.g. as is common in clinical decision support. The lack of intercept can be overcome provided sufficient information concerning the baseline survival curve is available (although this is rarely the case [17]). Once this is established, estimated survival probabilities are available. Censoring leads to problems in determining observed survival. This is commonly overcome by using Kaplan-Meier estimates [15], [18]. However, the censoring assumptions required for the Kaplan-Meier estimate are stronger than those required for the Cox model: the former requiring unconditional independence (random censoring), the latter requiring independence conditional on covariates only. This is a problem because when miscalibration is found using this approach, it is not clear whether this is genuine miscalibration or a consequence of the different censoring assumptions. Royston [19] presents an alternative approach for calibration at external validation. He uses the approach of pseudo-observations, as described by Perme and Anderson [20] to overcome the censoring issue and produce observed probabilities at individual level; however, this assumes that censoring is independent of covariates. In this paper and another [21] he proposes the comparison of KM curves in risk groups, which alleviates the strength of the independence assumption required for the censoring handling to be comparable between the Cox model and the KM curves (since the KM curves now only assume independent censoring within risk group). In these papers a fractional polynomial approach to estimating the baseline survival function (and thus being able to share it efficiently) is also provided. QRISK used the overall KM approach in the 2007 paper [18] with good results (6.34% predicted vs 6.25% observed in women and 8.86% predicted vs 8.88% observed in men), but bad results in the QRISK3 update [2] (4.7% predicted v 5.8% observed in women and 6.4% predicted vs 7.5% observed in men ). This may be because, as follow-up extends, the dependence of censoring on the covariates increases (QRISK had 12 years follow-up, QRISK3 18 years) and an important change between the update was the lower age limit moved from 35 to 25. A solution to this problem is to apply a weighting to uncensored patients based on their probability of being censored according to a model that accounts for covariates. The Inverse Probability of Censoring Weighting (IPCW) relaxes the assumption that patients who were censored are identical to those that remain at risk. The weighting inflates the patients who were similar to the censored population to account for those patients who are no longer available at a given time. Gerds &amp; Schumacher [22] have thoroughly investigated the requirements and advantages of applying an IPCW to a performance measure for modelling using the Brier score as an example and demonstrating the efficacy of its use, which was augmented by Spitoni et al [23] who demonstrated that any proper scoring rule can be improved by the use of the IPCW. This work has been added to by Han et al [24] and Liu et al [25] who demonstrated that the c-statistic is also suitable. In this paper we present an approach to assessing the calibration intercept (calibration-in-the-large) and calibration slope in time-to-event models based on estimating the censoring distribution, and reweighting observations by the inverse of the censoring probability. We first show, theoretically, how this method can be used and evidence that the metrics for calibration are amenable to its use. We then compare simulation results from using this weighted estimate to an unweighted estimate within various commonly used methods of calibration assessment. 5.2 Methods 5.2.1 Theory [Lots of Theory work on the probabilities involved from Matt] 5.2.2 Aims The aim of this study is to formalise the bias induced by applying different methods of assessing model calibration to data that is susceptible to censoring and to compare it to the bias when this data has been adjusted by the Inverse Probability of Censoring Weighting (IPCW). 5.2.3 Data Generating Method We simulated populations of patients with survival and censoring times, and took the observed event time as the minimum of these two values along with an event indicator of whether this was the survival or censoring time [26]. Each population was simulated with two parameters: \\(\\beta\\), \\(\\gamma\\) and \\(\\eta\\), which defined the proportional hazards coefficients for the survival and censoring distributions and the baseline hazard function, respectively. We varied the parameters to take all the values,\\(\\gamma = \\{-2,-1.5,-1,-0.5,0,0.5,1,1.5,2\\}\\), \\(\\beta = \\{-2,-1.5,-1,-0.5,0.5,1,1.5,2\\}\\) and \\(\\eta = \\{-\\;^{1}/_{2},0,\\;^{1}/_{2}\\}\\), that is the proportional hazard coefficients took the same values between -2 and 2, but \\(\\beta\\) did not take the value of 0 because this would make a predictive model infeasible. For each combination of parameters, we generated \\(N = 100\\) populations of \\(n = 10,000\\) patients (a high number of patients was chosen to avoid bias due to a small population size) with a single covariate \\(Z \\sim N(0,1)\\). For each patient, we then generated a survival time, \\(T\\) and a censoring time, \\(C\\). Survival times were simulated with a baseline hazard \\(\\lambda_0(t) = t^{\\eta}\\), and a proportional hazard of \\(e^{\\beta Z}\\). This allows the simulation of a constant baseline hazard (\\(\\eta = 0\\)) as well as an increasing (\\(\\eta = \\;^{1}/_{2}\\)) and decreasing hazard function Censoring times were simulated with a constant baseline hazard, \\(\\lambda_{C,0}(t) = 1\\) and a proportional hazard of \\(e^{\\gamma Z}\\). Once the survival and censoring times were generated, the event time, \\(X = \\min(T,C)\\), and the event indicator, \\(\\delta = I(T=X)\\), were generated. In the real-world, only \\(Z\\), \\(X\\) and \\(\\delta\\) would be observed. For each population, a prediction model for survival, \\(F_P\\) was chosen to be identical to the Data Generating Mechanism (DGM) to emulate a perfectly calibrated model: \\[ \\begin{array}{c} F_P(t|Z = z) = 1 - \\exp\\left(-\\frac{e^{\\beta Z}t^{\\eta+1}}{\\eta+1}\\right) \\end{array} \\] This prediction model was used to generate an estimate of the Expected probability that a given patient, with covariate \\(z\\), will have an event at the given time. To test the ability of approaches to detect miscalibration, we also derived a prediction model that would systematically over-estimate the prediction model, \\(F_O\\) and one which would systematically under-estimate the prediction, \\(F_U\\). These are defined as such: \\[ \\begin{array}{rl} F_U(t|Z=z) =&amp; \\textrm{logit}^{-1}\\left(\\textrm{logit}\\left( F_P(t|z) - 0.2\\right)\\right) \\end{array} \\] \\[ \\begin{array}{rl} F_O(t|Z=z) =&amp; \\textrm{logit}^{-1}\\left(\\textrm{logit}\\left( F_P(t|z) + 0.2\\right)\\right) \\end{array} \\] The prediction models were assessed at 100 time points, evenly distributed between the 25th and 75th percentile of observed event times, \\(X\\). At each time point, \\(t\\), we removed patients who had been censored (i.e. \\(T &lt; X_i\\) &amp; \\(\\delta_i = 0\\)) and created an indicator variable for whether each patient had had the event yet or not: \\[ \\begin{array}{c} O_i = I(X_i &lt; t\\;\\&amp;\\; \\delta_i = 1) \\end{array} \\] Similarly, we calculate a censoring prediction model, \\(G\\), to be identical to the DGM: \\[ \\begin{array}{c} G(t|z) = 1-\\exp\\left(-e^{\\gamma Z}t\\right) \\end{array} \\] This is used to calculate an IPCW for all non-censored patients at the last time they were observed (\\(t\\) for patients who have not had an event, and \\(X_i\\) for patients who have had the event), This is defined as: \\[ \\begin{array}{c} \\omega(t|z) = \\frac{1}{1 - G(\\min(t,X_i)|z)} \\end{array} \\] 5.2.4 Methods At each of these time points, we compare Observed outcomes (\\(O\\)) with the Expected outcomes (\\(E\\)) of the prediction models based on four choices of methodology [6], [19], [21], [27] to produce measures for the calibration-in-the-large Kaplan-Meier (KM) - A Kaplan-Meier estimate of survival is estimated from the data and the value of the KM curve at the current time is taken to be the average Observed number of events within the population and this is compared with the average Expected value. Logistic Unweighted (LU) - Logistic regression is performed on the non-censored population to predict the binary Observed value using the logit(Expected) value as an offset and the Intercept of the regression is the estimate. Logistic Weighted (LW) - As above, but the logistic regression is performed using the IPCW as a weighting for each non-censored patient. Pseudo-Observations (PO) - The contribution of each patient (including censored patients) to the overall Observed is calculated by removing them from the population and aggregating the difference. Logistic regression is performed using the log cumulative hazard as an offset and the Intercept of the result is the estimate. The weights within the LW method create a non-integer number of events within the regression and the PO method can produce values that are not always 0 or 1 (as would be expected in an ordinary logistic regression). The values produced by PO will have to be artificially capped between 0 and 1, but otherwise these two methods do not cause any issues. 5.2.5 Estimands For each set of parameters and methodology, our estimand at time, \\(t\\), measured in simulation \\(i = 1,...,N\\) is \\(\\theta_i(t)\\), the set of estimates of the calibration-in-the-large for the \\(F_P\\), \\(F_U\\) and \\(F_O\\) models in order. Therefore our underlying truth for all time points is \\[\\begin{array}{c} \\theta = \\left(0,0.1,-0.1\\right) \\end{array}\\] From this, we can also define our upper and lower bound for a 95% confidence interval as the vectors \\(\\theta_{i,L}(t)\\) and \\(\\theta_{i,U}(t)\\). 5.2.6 Performance Measures The measures we will take as performance measures as the Bias, the Empirical Standard Error as the Coverage at time, \\(t\\), along with relevant standard errors and confidence intervals as per current recommendations [28]. These measures can be seen in table 5.1. Performance MeasureEstimationSEBias$$\\hat{\\theta}(t) = \\frac{1}{N}\\sum_{i=1}^N\\theta_i(t)-\\theta$$$$\\hat{\\theta}_{SE}(t) = \\sqrt{\\frac{1}{N(N-1)}\\sum_{i=1}^N\\left(\\theta_i(t) - \\hat{\\theta}(t)\\right)^2}$$EmpSE$$\\hat{E}(t) = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^N\\left(\\theta_i(t) - \\hat{\\theta}\\right)^2}$$$$\\hat{E}_{SE}(t) = \\frac{\\hat{E}(t)}{\\sqrt{2(N-1)}}$$Coverage$$\\hat{C}(t)=\\frac{1}{N}\\sum_{i=1}^NI\\left(\\theta_{i,L}(t) \\le \\theta\\le\\theta_{i,U}(t)\\right)$$$$\\hat{C}_{SE}(t) = \\frac{\\hat{C}(t)(1-\\hat{C}(t))}{N}$$ Table 5.1: Description of the Performance Measures defined at time \\(t\\) For each estimand above, \\(\\hat{Q}(t) = \\{\\hat{\\theta}(t),\\hat{E}(t), \\hat{C}(t)\\}\\) and associated SE, \\(\\hat{Q}_\\textrm{SE}(t) = \\{\\hat{\\theta}_\\textrm{SE}(t),\\hat{E}_\\textrm{SE}(t), \\hat{C}_\\textrm{SE}(t)\\}\\), we average over time. As these measures will be taken at each of the 100 time points, \\(t_j:j=1...100\\), we summarise each of these measures as an average and as weighted average, as seen in table 5.2. The weight used for the measure at time \\(t_j\\) is the average number of non-censored patients remaining in the population at time \\(t_j\\), defined as \\(n_j\\) (note that this includes patients who have had the event). EstimandSEUnweighted$$\\bar{Q}=\\frac{1}{100}\\sum_{j=1}^{100}\\hat{Q}(t_j)$$$$\\bar{Q}_{SE}=\\sqrt{\\frac{1}{100}\\sum_{j=1}^{100}\\hat{Q}_{SE}(t_j)^2}$$Weighted$$\\bar{Q}'=\\frac{\\sum_{j=1}^{100}n_j\\hat{Q}(t_j)}{\\sum_{j=1}^{100}n_j}$$$$\\bar{Q}_{SE}^'=\\sqrt{\\frac{\\sum_{j=1}^{100}n_j\\hat{Q}_{SE}(t_j)^2}{\\sum_{j=1}^{100}n_j}}$$ Table 5.2: Description of how Performance Measures are averaged over time 5.2.7 Software All analysis was done in R 3.6.3 [29] using the various tidyverse packages [30], Kaplan-Meier estimates were found using the survival package [31], Pseudo-Observations were evaluated with the pseudo package [32]. 5.3 Results Forthcoming 5.4 Discussion Weighting = Good. Not Weighting = Bad. 5.5 References 5.6 Supplementary Material - Calibration Slope The main purpose of this paper was to assess the evaluation of calibration-in-the-large at different time points in a time-to-event clinical prediction model. Along with calibration-in-the-large, various methods of calibration can also produce measures of calibration slope. Calibration slope provides an insight into how well the model predicts outcomes across the range of predictions. In an ideal model, the calibration slope would be 1. The Logistic Weighted, Logistic Unweighted and Pseudo-Observation methods described above can provide estimates of the calibration slope. For each of these methods, we first estimate the calibration-in-the-large as above, using a predictor as an offset, then we use this estimate as an offset to predict the calibration slope (without an intercept term). 5.6.1 Results blah blah 5.6.2 Discussion Brief discussion, much briefer than the main points. "],
["chap-performance-metrics.html", "Chapter 6 Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models 6.1 Introduction 6.2 Motivating Data Set 6.3 Current Approaches 6.4 Extension to Multi-State Models 6.5 Application to Real-World Data 6.6 Discussion", " Chapter 6 Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models 6.1 Introduction Clinical Prediction Models (CPMs) provide individualised risk of a patient’s outcome (cite), based on that patient’s predictors. These predictions will usually be in the form of a risk score or probability. However, using traditional modelling techniques, these CPMs will only predict a single outcome. Multi-State Clinical Prediction Models (MS-CPMs) combine the multi-state modelling framework to the prognostic field to provide predictions for multiple outcomes in a single model. Once a CPM has been developed, it is important to assess how well the model actually performs (cite). This process is called Model Validation and involves comparing the predictions produced by the model to the actual outcomes experienced by patients (cite). It is expected that the development of a CPM will be accompanied by the validation of the model on the same dataset it was developed in (internal validation), using either bootstrapping or cross-validation to account for optimism in the developed model (cite). Models can also be validated on a novel dataset (external validation), which is used to assess the generalisability and transportability of the model (cite). During validation, there are different aspects of model performance that we can assess and these are measured using specific metrics. For example, to assess the overall Accuracy of a model, we may use the Brier Score (cite) or to analyse how well a model discriminates between patients, we could use the c-statistic (cite). The current metrics that are commonly used have been designed and extended to work in a variety of model development frameworks. However, these extensions are limited to either a single outcome (as in traditionally developed models) or do not adequately account for the censoring of patients (as commonly occurs in longitudinal data). This paper aims to provide use-able extensions to current performance metrics to be used when validating MS-CPMs. It is essential that these extensions are directly comparable with current metrics (to allow for quicker adoption), that they are collapsible to the current metrics and that they adjust for the bias induced by the censoring of patients. Currently, the most common way to validate an MS-CPMs is by applying traditional methods to compare across two states at a given time and then aggregating the results in an arbitrary manner [cite something]. Other methodologists have extended existing metrics to multinomial outcomes [cite van Calster], which do not contain a time-based component; to simple competing risks scenarios [cite CR c-statistic], which do not contain transient states; or to [… insert third relevant example]. Spitoni et al [cite Spitoni 2018]] developed methods to apply the Brier Score (or any proper score functions) to a multi-state setting and so a simplified and specific version of their work is described in this paper. It is the hope of the authors that this work will increase the uptake of multi-state models and the sub-field of MS-CPMs will grow appropriately. 6.2 Motivating Data Set [Table One for The Glasgow Data] Throughout this paper we will use a model developed in Chronic Kidney Disease (CKD) patients to assess their progression onto Renal Replacement Therapy (RRT) and/or Death [cite Dev/Valid Paper]. The model was developed using data from the Salford Kidney Study (SKS) and then applied to an external dataset derived from the West of Scotland (see Table 2) [1]. The original model predicts the probability that a patient has begun RRT and/or died after their first recorded eGFR below 60 ml/min/1.73m2, by any time in the future (reliable up to 10 years). For the purposes of this paper, we will take a “snapshot” of the predictions at the 5 year time point. The Three-State model used in our example is designed as an Illness-Death Model [2], this is one of the simplest MSM designs and has the key advantage over a traditional model that they can predict whether a patient is in or has visited the transient state before reaching the absorbing state (i.e. patient who became ill before dying or who started RRT before dying) (see figure 1). [Figure of the MSM] [Describe Glasgow Data] 6.3 Current Approaches Here we describe three commonly used performance metrics for assessing the performance of a traditional survival clinical prediction model. These metrics assess the Accuracy, Discrimination and Calibration of the models being validated. Accuracy is an overall measurement of how well the model predicts the outcomes in the patients. Discrimination assesses how well the model discerns between patients; in a two-state model this is a comparison of patients with and without the outcome, and should assign a higher value to those that experience the outcome. Calibration is the agreement between the observed outcomes and the predicted risks across the full risk-range. We are applying cross-sectional metrics at a set time point within the setting of a longitudinal model and so we need to account for the censoring of patients and therefore, each uncensored patient at a given time t will be weighted as per the Inverse Probability of Censoring Weighting (IPCW) [3]. This allows the uncensored patient population to be representative of the entire patient population. 6.3.1 Baseline Models To assess the performance of a model, we must compare the values produced by the performance metrics to those of two baseline models; a random or noninformative model and a perfect model. A Non-Informative (NI-)model assigns the same probability to all patients to be in any state regardless of covariates and is akin to using the average prevalence in the entire population to define your model. For example, in a Two-State model and an event that occurs in 10% of patients, all patients are predicted to have a 10% chance of having the event. For many metrics, models can be compared to a Non-Informative model to assess whether the model is in fact “better than random”. A Perfect (P-)model is one which successfully assigns a 100% probability to all patients, and the predictions are correct; this is the ideal case, which many models can also be compared to as models as close to this display excellent predictive abilities. Although models may perform worse than a non-informative one, we will not consider these in detail here as they are considered to be without worth in terms of predictive ability. The metrics produced by these baseline models will often depend on the prevalence of each state and/or the number of states. These values can be used as comparators to provide contextual information regarding the strength of model performance. These baselines metrics for the NI-model and the P-model will be referred to as the NI-level and P-level for the metric. In order to allow for simplicity and understanding of these measures, they will be standardised to the same scales. 6.3.2 Notation Throughout this paper, we will use consistent notation which is shown here for reference and to avoid repetition in definitions, etc… [Notation Table] 6.3.3 Patient Weighting [Lots of formula, so will leave for now] 6.3.4 Accuracy - Brier Score 6.3.5 Discrimination - c-statistic 6.3.6 Calibration - Intercept and Slope 6.4 Extension to Multi-State Models 6.4.1 Trivial Extensions 6.4.2 Accuracy - Multiple Outcome Brier Score 6.4.3 Discrimination - Polytomous Discriminatory Index 6.4.3.1 Computational Limitations 6.4.4 Calibration - Multinomial Intercept, Matched and Unmatched Slopes 6.5 Application to Real-World Data 6.5.1 Accuracy 6.5.2 Discrimination 6.5.3 Calibration 6.6 Discussion "],
["chap-dev-paper.html", "Chapter 7 Development and External Validation of a Multi-State Clinical Prediction Model for Chronic Kidney Disease Patients Progressing onto Renal Replacement Therapy and Death 7.1 Introduction 7.2 Methods 7.3 Results 7.4 Discussion", " Chapter 7 Development and External Validation of a Multi-State Clinical Prediction Model for Chronic Kidney Disease Patients Progressing onto Renal Replacement Therapy and Death 7.1 Introduction Chronic Kidney Disease (CKD) is a progressive disease that affects the ability of the kidneys to filter toxins from the blood. Patients with End-stage Renal Disease (ESRD) are treated using Renal Replacement Therapy (RRT), which collectively describes the treatments designed to emulate the processes performed by the failing kidneys. The three most common treatment modalities are haemodialysis (HD), peritoneal dialysis (PD) and kidney transplant (Tx). The more severe stages of CKD (stages 3 - 5) affects approximately 2.6 million people over the age of 16 in England [33] with around 63 thousand adult patients registered for RRT in 2015 [34], of which 8 thousand were new patients [35]. A prognostic model is a tool which provides patients and clinicians with a measure of how likely a patient is to suffer a specific clinical event in the future [8]. They use data from previous patients to estimate the outcomes of an individual patient. Prognostic models are used in clinical practice to influence treatment decisions such as the prescribing of statins for cardiovascular disease via the application of the QRISK models [2]. Within CKD, prognostic models have been developed to predict mortality [36]–[40], ESRD [37], the commencements of RRT [39], [41]–[43] or mortality after beginning dialysis [44]–[46]. Some previous models have used the commencement of RRT as a proxy for ESRD [47]–[49], while others have investigated the occurrence of cardiovascular events, which are common amongst CKD patients[50]–[52]. Reviews by Grams &amp; Coresh [53], Tangri et al [54] and Ramspek et al [55], which explored the different aspects of assessing risk amongst CKD or RRT patients, found that the current landscape of CKD prediction models is lacking from both a methodological and clinical perspective [56], [57]. Methodologically, the majority of existing CKD prediction models fail to account for completing events [38], [40], [58], have high risks of bias [36], [37], [41] or are otherwise flawed compared to modern clinical prediction standards [8], [56] In 2013, Begun et al [59] developed a multi-State model for assessing population-level progression through the severity stages of CKD (III-V), RRT and/or death, which can be used to provide a broad statement regarding a patient’s future. In 2014, Allen et al [60] applied a similar model to liver transplant recipients and their progression through the stages of CKD with a focus on the predictions of measured vs estimated glomerular filtration rate (mGFR vs eGFR). In 2017, Kulkarni et al [43] developed an MSM focusing on the categories of Calculated Panel Reactive Antibodies (CPRA) and kidney transplant and/or death. Most recently, in 2018, Grams et al [61] developed a multinomial clinical prediction model for CKD patients which focused on the occurrence of RRT and/or cardiovascular events. As of the publication of this paper, this is the only currently existing CPMs of this kind for CKD patients. However, the first three of these existing models (Begun, Allen and Kulkarni) categorise continuous variables to define their states at specific cut-offs and this has been shown to be inefficient when modelling [62]–[80]. These kinds of cut-offs can be useful when informing patients and clinicians of a patient’s diagnosis and to coincide with policy, but inherently cause a loss of information when done before the data analysis stage and so these models go against current statistical recommendations. These kinds of assumptions are also subject to measurement error [81] and interval censoring [82], i.e. we do not know when exactly when a patient moved from CKD Stage III to CKD Stage IV, or whether drop in estimated Glomerular Function Rate (eGFR) was temporary or inaccurate. For example, Kulkarni [43] assumes that a patient with an CPRA of (5%) is the same as a patient with an CPRA of (75%) and that a patient with an CPRA of (89.9%) is vastly different from a patient with an CPRA of (90%). Moreover, none of these models have undergone any validation process, whether internal or external [10]. It is also important to note that although these models can be used to predict patient outcomes, these models were not designed to produce individualised patient predictions as is a key aspect of a clinical prediction model; they were designed to assess the methodological advantages of MSMs in this medical field, to describe the prevalence of over time of different CKD stages and to produce population level predictions for patients with different levels of panel-reactive antibodies [9]. The fourth model (Grams), is presented as a Multi-State Model and the transitions involved were studied and defined, however the underlying statistical model is a pair of multinomial logistic models analysed at 2 and 4 years. The major downside of this model is that it can only produce predictions at those predefined time points and it assumes homogeneity of transition times. For example, the first model assumes that a patient who began RRT 1 month after study entry is the same as one who began after 1 year &amp; 11 months into the study and then the second model assumes these patients are the same as one who begins RRT at 3 years and 11 months. Therefore, the aim of this study was to improve on previous efforts to model a patient’s pathways through a Multi-State Model by choosing transition points which can be exactly identified and include states which produce a drastic difference in patient characteristics. Our modeling techniques allow for individual predictions (using a proportional hazards model) of multiple outcomes (using MSMs) at any time point (using cubic splines). The models produced by this process will then be validated, both internally and externally, to compare their results and demonstrate the transportability of the (statistically robust) clinical prediction models. We report our work in line with the TRIPOD guidelines for development and validation of clinical prediction models [56], [83]. 7.2 Methods 7.2.1 Data Sources The models were developed using data from the Salford Kidney Study (SKS) cohort of patients (previously named the CRISIS cohort), established in the Department of Renal Medicine, Salford Royal NHS Foundation Trust (SRFT). The SKS is a large longitudinal CKD cohort recruiting CKD patients since 2002. This cohort collects detailed annualised phenotypic and laboratory data, and plasma, serum and whole blood stored at -80C for biomarker and genotypic analyses. Recruitment of patients into SKS has been described in multiple previous studies [84], [85] and these have included a CKD progression prognostic factor study and to evidence the increased risk of cardiovascular events in diabetic kidney patients. In brief, any patient referred to Salford renal service (catchment population 1.5 million) who is 18 years or over and has an eGFR measurement of less than \\(60\\textrm{ml}/\\textrm{min}/1.73\\textrm{m}^2\\) (calculated using the CKD-EPI formula [86]) was approached to be consented for the study participation. At baseline, the data, including demographics, comorbidities, physical parameters, lab results and primary renal diagnosis are recorded in the database. Patients undergo an annual study visit and any changes to these parameters are captured. All data except blood results are collected via questionnaire by a dedicated team of research nurses. Blood results (baseline and annualised), first RRT modality and mortality outcome data are directly transferred to the database from Salford’s Integrated Record (SIR) [87]. eGFR, uPCR, comorbidity and blood results were measured longitudinally throughout a patient’s time within the cohort. Due to limitations in our data, we were agnostic to how long since patients were diagnosed with CKD. Therefore, we defined a patient’s start date for our model as their first date after consent at which their eGFR was recorded to be below \\(60\\textrm{ml}/\\textrm{min}/1.73\\textrm{m}^2\\). Some patients consented with an eGFR that was already below 60, and some entered our study later when their eGFR was measured to be below 60. This implies that our models includes both patient who have recently been diagnosed with CKD (\\(\\textrm{eGFR} \\lessapprox 60\\)) and those that have been suffering with CKD for an arbitrary amount of time. This timelessness of the model means it can be applied to any patient at any time during their CKD journey. This allows for a wider range of baseline eGFR measurements and patients who have been suffering from CKD, translating to a model which can be applied to All patients registered in the database between October 2002 and December 2016 with available data were included in this study. As this is a retrospective convenience sample, no sample size calculations were performed prior to recruitment. All patients were followed-up within SKS until the end-points of RRT, death or loss to follow-up or were censored at their last interaction with the healthcare system prior to December 2017. Date of death for patients who commenced RRT was also available within SIR and so also included in the SKS database. For external validation of the model, we extracted an independent cohort from the West of Scotland Electronic Renal Patient Record (SERPR). Our extract of SERPR contains all patients known to the Glasgow and Forth Valley renal service who had an eGFR measure of less than \\(60\\textrm{ml}/\\textrm{min}/1.73m^2\\) between January 2006 and January 2016. This cohort has been previously used in Chronic Kidney Disease Prognosis consortium studies investigating outcomes in patients with CKD [88] and a similar cohort has been used for the analysis of skin tumours amongst renal transplant patients. Use of anonymised data from this database has been approved by the West of Scotland Ethics Committee for use of NHS Greater Glasgow and Clyde ‘Safe Haven’ data for research. Both the internal and external validation cohort were used as part of the multinational validation cohort used by Grams et al in their multinomial CPM discussed above [61]. In SERPR, start dates were calculated to be the first time point where the following conditions were met: eGFR is measured at less than 60 There is at least one prior eGFR measurement Patient is 18 or over Patient is not enduring an AKI [89], [90]. The second requirement was implemented to avoid a bias in the eGFR Rate. eGFR Rate is a measure of the change in eGFR over time and is calculated as the difference between the most recent two eGFR measurements divided by the time between them. For patients who entered the system with an \\(\\textrm{eGFR} &lt; 60\\), their eGFR Rate would be unavailable (i.e. missing). Otherwise, patient eGFRs would have to drop to below 60 and thus eGFR Rate would be negative. [I feel like this bias should have a name, but can’t think what to search to find it] 7.2.2 Development Three separate models were developed, so we could determine a clinically viable model while maintaining model parsimony as much as possible: a Two-State, Three-State and Five-State model, each building on the previous models’ complexity (see figure 7.1). The Two-State model was a traditional survival analysis where a single event (death) is considered. The Three-State model expanded on this, by splitting the Alive state into transient states of (untreated) CKD and (first) RRT; patients can therefore transition from CKD to Death or CKD to RRT, and then onto RRT to Death. The Five-State model stratifies the RRT state into HD, PD and Tx and allows similar transitions into and out of the RRT states; however, the transition from Tx to Death was not considered as it was anticipated a priori that there would be insufficient patients undergoing this transition and that the process of undergoing a transplant would be medically transformative and so it would be inappropriate to assume shared parameters before and after the transition (i.e. Tx was modelled as a second absorbing state). Figure 7.1: Diagram of the three models, the states being modelled and relevant transitions Data was recorded in a time-updated manner, however all variables were measured at baseline to emulate the real-world application of the model (i.e. future prediction of states and not covariates). Variables considered as covariates were demographics (sex, age, smoking status and alcohol consumption), comorbidities (congestive cardiac failure (CCF), chronic obstructive pulmonary disease (COPD), prior cerebrovascular accident (CVA), hypertension (HT), diabetes mellitus (DM), ischemic heart disease (IHD), chronic liver disease (LD), prior myocardial infarction (MI), peripheral vascular disease (PVD) and slid tumour (ST)), physical parameters (BMI, blood pressure), blood results (haemoglobin, albumin, corrected calcium and phosphate measures), urine protein creatinine ratio (uPCR) and primary renal diagnosis (grouped as per ERA-EDTA classifications [91]). Ethnicity was assessed in the populations, but as most patients were white, it was omitted as a potential predictor from the models. uPCR and eGFR Rate of change were also calculated [92], [93] as the difference between the two most recent measures divided by time difference in years. \\(\\textrm{Age}^2\\), log(Age), log(eGFR Rate) and log(uPCR Rate) were considered as transformations within the model. log(Calendar Time) was included as a covariate to adjust for time trends in treatment preferences [Cite: Something]. Calendar Time was defined as length of time between start date and 1st January 2019. [Matt mentioned the use of log Calendar Time rather than simply Calendar Time. I read this a while ago and can’t currently locate the paper (I believe it was a simulation study). I’ll carry on looking for it though of course] Intermediate states (RRT or modality) were considered to be medically transformative, and so a semi-markov (clock reset) method for analysis was considered to be well justified [94]. Each transition was modelled under a proportional hazards assumption using the Royston-Parmar technique [95] to estimate coefficients for each covariate and a restricted cubic spline (on the log-time scale) for the baseline cumulative hazard. The cumulative hazards for each transition can be combined to produce estimates for the probability of a patient being in any state at any time [96]. For variable selection, we stacked the imputed datasets together to create a larger, pseudo-population [97] and performed backwards-forwards selection based on minimising the AIC at each step. This was repeated for each transition and for different numbers of evenly spaced knots in modelling the form of the baseline hazard, K={0,1,2,3,4,5}. This allowed for different transitions to use different sets of variables and numbers of knots in the final model. Some combinations of variables resulted in models that were intractable and so these models were excluded. Once a set of variables were chosen, the R-P model was applied to each imputed dataset individually and the resulting coefficients and cubic spline parameters were aggregated across imputations using Rubin’s Rules [98]. This gave a model fully defined by smooth cubic splines representing the cumulative cause-specific hazard and individualised proportional hazards for each transition. All missing data were assumed to be missing at random and so were multiply imputed using chained equations with the Nelson-Aalen estimators for each relevant transition as predictors [99]. Some variables (smoking status and histories of COPD, LD and ST) were present in the SKS (development) dataset, but were completely missing in the SERPR extract (validation) and so these were multiply imputed from the development dataset [100]. 7.2.3 Example Once the models have been developed, we will apply them to two example patients to demonstrate their use and applicability to the general population. We will provide a direct clinical estimation of these patient outcomes based on years of nephrological experience and compare this with the results presented by our clinical prediction model. We have chosen two (artificial) patients to use as examples of the use of our model. Their details can be seen below: Patient 1Patient 250 Year Old Female25 Year Old Male1.60m Tall and 80 kg (BMI: 31.25)1.75m Tall and 75 kg (BMI: 24.5)Never SmokedQuit Smoking more than 3 years agoBP: 130/85BP: 130/80Blood Levels:Blood Levels:Albumin : 44 g/lAlbumin : 48 g/lCorrected Calcium : 2.3 mmol/lCorrected Calcium : 2.4 mmol/lHaemoglobin : 140 g/lHaemoglobin : 150 g/lPhosphate : 1.00 mmol/lPhosphate : 0.85 mmol/leGFR: 50 (Rate: -3.0/year)eGFR: 59 (Rate: 10.0/year)uPCR: 0.2 (Rate: -0.5/year)uPCR: 0.1 (Rate: -70.0/year)Renal Diagnosis:Renal Diagnosis:Systemic diseases affecting the kidneyMiscellaneous renal disordersComorbidities:Comorbidities:NoneCongestive Cardiac FailureCerebrovascular AccidentMyocardial InfarctionIschemic Heart DiseasePeripheral Vascular DiseaseLiver DiseaseSolid TumourHypertension Table 7.1: Details of the Example Patients The main differences between our two patients are the difference in age (Patient 1 is twice as old as Patient 1), Patient 2 is a former smoker, but their blood measures are fairly similar with just minor differences. Patient 1’s eGFR has dropped to \\(50\\textrm{ml}/\\textrm{min}/1.73\\textrm{m}^2\\), whereas Patient 2’s eGFR has risen to \\(59\\textrm{ml}/\\textrm{min}/1.73\\textrm{m}^2\\). Patient 2 has quite a few comorbidities, but Patient 1 has none. Patient 2’s uPCR has dropped dramatically recently. The etiology of their kidney disease is also different. 7.2.4 Validation Further to the simple individual validation performed above. A statistical validation will need to be performed to formally establish the performance of the models. Each of the three models were internally validated in the development dataset using bootstrapping to adjust for optimism and then further externally validated in the validation dataset extracted from SERPR. The bootstrapping method was also used for both validations to produce confidence intervals around the performance metric estimates. To assess the performance in low eGFR patients, the models were also validated in subsets of the SKS and SERPR where patients had an \\(\\textrm{eGFR} &lt; 30\\textrm{ml}/\\textrm{min}/1.73\\textrm{m}^2\\). For validation purposes, we consider Death and Death after RRT/HD/PD to be distinct states meaning that for the Three-State model, we have \\(K=4\\) pathways a patient can take and for the Five-State model, we have \\(K=7\\). To compare across models, we combined states together to collapse down to simpler versions. We collapsed the Three-State model to a two-state structure by combining the CKD and RRT states into an Alive state. We collapsed the Five-State model to a three-state structure by combining the HD, PD and Tx into an RRT state and then further down to a two-state structure as with the Three-State model. We will report performance measures at 1-year, 2-years, 5-years and 8-years. As well as presenting the performance measures over time. The overall accuracy of each model was assessed using the MSM adjusted Brier Score [Cite: Performance Metrics], which is a proper score function assigning 0 to a non-informative model and 1 to a perfect model, with negative numbers implying the model performs worse than assuming every patient’s state predictions are the same as the overall prevalence within the population. The discrimination of each model was assessed using the MSM extension to the c-statistic [101] [Cite: Performance Metrics]. The c-statistic is a score between 0 and 1 with higher scores suggesting a better model and a c-statistic of 0.5 suggesting the model performs no better than a non-informative model. The calibration of each model was assessed using MSM multinomial logistic regression (MLR) [102] [Cite: Performance Metrics] which extends the logistic regression to three or more mutually exclusive outcomes [6]. This produces an intercept vector of length \\(K-1\\) and a Slope-matrix of dimension \\((K-1) \\times (K-1)\\). As with the traditional calibration intercept for a well performing model, the MLR intercept values should all be as close to 0 as possible. The traditional calibration slope should be as close to 1 as possible and so the multi-state extension of the slope, the Slope-matrix should be as close to the identity matrix (\\(I\\)) as possible. 7.2.5 Calculator As part of this work, we also intend to produce an online calculator to allow patients and clinicians to easily estimate outcomes without worrying about the mathematics involved. All analysis was done in R 3.6.2 [29] using the various tidyverse packages [30], as well as the mice [103], flexsurv [104], nnet [105] and furrr [106] packages. The calculator was produced using the shiny package [107]. 7.3 Results 7.3.1 Data Sources As seen in table 7.2, The Age of both populations were centred around 64-65 with a very broad range. Due to the inclusion criteria, eGFR were capped at a maximum of 60, and was consistent across populations; however, the rate of change for eGFR was much wider in the SERPR patients than in the SKS, and it was decreasing much faster, on average ( -25 vs 0) . Blood pressure was also consistent across populations (140/75 vs 148/76 for development vs validation). The blood test results (Corrected Calcium, Albumin, Haemoglobin and Phosphate) was close together, with the further difference being Haemoglobin with an average of 123 in SKS and 109 in SERPR and a much larger standard deviation in SERPR compared to SKS (38 vs 17). Similar to the eGFR measures, the uPCR results were similar, but the rates of change were much broader in the validation dataset compared to the SKS and were generally increasing, whereas SKS remained stationary (73 vs 0). Levels of missingness were much higher in the SERPR dataset in most continuous variables. VariableSKS (Development)SERPR (Validation)Age64.378 (14.573) [ 20.000, 94.000] &lt; 0 ( 0.00%)&gt;65.880 ( 13.734) [ 18.000, 98.000] &lt; 0 ( 0.00%)&gt;eGFRa30.368 (14.122) [ 3.577, 59.965] &lt; 0 ( 0.00%)&gt;36.132 ( 13.668) [ 1.651, 59.998] &lt; 0 ( 0.00%)&gt;eGFR Ratea-0.015 ( 1.528) [-19.107, 33.781] &lt;1,278 (42.87%)&gt;-25.476 (623.048) [-8,755.272, 9,260.375] &lt; 0 ( 0.00%)&gt;SBPb140.193 (21.839) [ 77.000, 220.000] &lt; 50 ( 1.67%)&gt;147.746 ( 25.984) [ 82.000, 258.000] &lt;6,880 (88.61%)&gt;DBPb74.555 (11.754) [ 36.000, 159.000] &lt; 52 ( 1.74%)&gt;76.263 ( 13.853) [ 35.000, 128.000] &lt;6,879 (88.60%)&gt;BMIc28.848 ( 6.114) [ 13.182, 61.466] &lt; 572 (19.18%)&gt;29.331 ( 6.722) [ 15.343, 48.301] &lt;7,681 (98.93%)&gt;Albumind42.152 ( 4.186) [ 12.000, 52.000] &lt; 60 ( 2.01%)&gt;36.490 ( 5.647) [ 7.000, 53.000] &lt;3,455 (44.50%)&gt;Corrected Calciume2.302 ( 0.147) [ 1.209, 3.660] &lt; 68 ( 2.28%)&gt;2.408 ( 0.158) [ 1.419, 3.610] &lt;5,113 (65.85%)&gt;Haemoglobind122.977 (17.068) [ 61.000, 195.000] &lt; 72 ( 2.41%)&gt;108.588 ( 38.446) [ 6.250, 208.000] &lt;3,968 (51.10%)&gt;Phosphatee1.162 ( 0.284) [ 0.430, 3.710] &lt; 87 ( 2.91%)&gt;1.203 ( 0.311) [ 0.370, 4.370] &lt;5,127 (66.03%)&gt;uPCRf0.112 ( 0.195) [ 0.000, 2.025] &lt; 245 ( 8.21%)&gt;0.184 ( 0.477) [ 0.000, 6.390] &lt;7,513 (96.76%)&gt;uPCR Ratef-0.096 ( 3.035) [-70.727, 28.198] &lt;1,777 (59.61%)&gt;73.177 (465.186) [ -2.255, 3,051.403] &lt;7,721 (99.44%)&gt;a(ml/min/1.73m^2) or per year; b(mmHG); c(kg/m^2); d(g/l); e(mmol/l); f(gmol/l) or per year Table 7.2: Population demographics for the continuous variables presented as: mean (sd) [min,max] &lt;number missing (percent missing)&gt; Table 7.3 shows a breakdown of the categorical variables across the populations. In the development population, there are far more males than females, whereas in the validation population the proportions are much more matched. Most patients were white in the SKS dataset, and ethnicity has extremely high missingness in SERPR, which also contributed to its omission from the model. The majority of the SKS patients were former smokers, however this information was unavailable in the SERPR dataset. Primary Renal Diagnosis suffered from very high levels of missingness in the validation dataset, but was much better recorded in the development dataset (although still far from perfect). VariableCategorySKS (Development)SERPR (Validation)GenderMale1,865 (62.56 %)3,915 (50.42 %)Female1,116 (37.43 %)3,849 (49.57 %)EthnicityWhite2,875 (96.44 %)683 ( 8.79 %)Asian75 ( 2.51 %)12 ( 0.15 %)Black21 ( 0.70 %)7 ( 0.09 %)Other10 ( 0.33 %)2 ( 0.02 %)&lt;Missing&gt;0 ( 0.00 %)7,060 (90.93 %)Smoking StatusFormer1,535 (51.49 %)Non-Smoker979 (32.84 %)Smoker379 (12.71 %)Former 3Y46 ( 1.54 %)&lt;Missing&gt;42 ( 1.40 %)Renal DiagnosisSystemic diseases affecting the kidney1,304 (43.74 %)299 ( 3.85 %)Glomerular disease442 (14.82 %)225 ( 2.89 %)Tubulointerstitial disease268 ( 8.99 %)164 ( 2.11 %)Miscellaneous renal disorders227 ( 7.61 %)188 ( 2.42 %)Familial / hereditary nephropathies173 ( 5.80 %)102 ( 1.31 %)&lt;Missing&gt;567 (19.02 %)6,786 (87.40 %) Table 7.3: Population demographics for the categorical variables presented as number (percent) Overall, there were high levels of comorbidities within the SKS population as shown in table 7.4, but these levels were much lower in the SERPR population, possibly due to the data extraction processed (where data is un-recorded, no history is assumed). In SKS, most comorbidities were at over 80% prevalence, apart from diabetes mellitus, which had a lower prevalence of 33% and over 97% (2,891) patients had a history of liver disease. In SERPR, hypertension was the highest prevalence in SERPR at 40% (3,122), followed by diabetes mellitus at 20% (1,546) and cerebrovascular accident was the lowest prevalence at 2.36% (184). Liver disease, chronic obstructive pulmonary disease and solid tumour data were unavailable in the SERPR data. VariableSKS (Development)SERPR (Validation)DM992 (33.32%) &lt; 4 (0.13%)&gt;1,546 (19.91%) &lt; 0 ( 0.00%)&gt;CCF2,414 (81.08%) &lt; 4 (0.13%)&gt;406 ( 5.22%) &lt; 0 ( 0.00%)&gt;MI2,492 (83.70%) &lt; 4 (0.13%)&gt;556 ( 7.16%) &lt; 0 ( 0.00%)&gt;IHD2,393 (80.38%) &lt; 4 (0.13%)&gt;867 (11.16%) &lt; 0 ( 0.00%)&gt;PVD2,485 (83.47%) &lt; 4 (0.13%)&gt;376 ( 4.84%) &lt; 0 ( 0.00%)&gt;CVA2,727 (91.60%) &lt; 4 (0.13%)&gt;184 ( 2.36%) &lt; 0 ( 0.00%)&gt;COPD2,411 (80.98%) &lt; 4 (0.13%)&gt;LD2,891 (97.11%) &lt; 4 (0.13%)&gt;ST2,570 (86.32%) &lt; 4 (0.13%)&gt;HT2,546 (91.48%) &lt;198 (6.64%)&gt;3,122 (40.21%) &lt; 0 ( 0.00%)&gt; Table 7.4: Population comorbidity prevalence for the two populations presented as number (percent) &lt;number missing (percent missing)&gt; The median date for the date of death was 3.9 years in the SKS population and 4.9 years in the SERPR population. The median date for transition to RRT was 2.2 years and 1.5 years (in SKS and SERPR respectively). In SKS, transitions to HD happened 6 months later than PD, and in SERPR it was 3.6 months. The Maximum followup time in SKS was 15.0 years and in SERPR it was 10.1 years. This information can be seen in table 7.5. ModelFromToSKS (Development)SERPR (Validation)TwoAliveDead1,427 = 3.9 y (4.3 y) [0.0 y, 15.0 y]3,025 = 4.9 y (3.3 y) [0.0 y, 10.1 y]ThreeCKDDead1,125 = 3.5 y (4.2 y) [0.0 y, 15.0 y]2,579 = 4.8 y (3.2 y) [0.0 y, 10.1 y]RRT680 = 2.5 y (3.3 y) [0.0 y, 14.1 y]1,130 = 3.8 y (3.8 y) [0.0 y, 10.1 y]RRTDead302 = 2.2 y (3.2 y) [0.0 y, 13.5 y]446 = 1.5 y (2.4 y) [0.0 y, 9.1 y]FiveCKDDead1,125 = 3.5 y (4.2 y) [0.0 y, 15.0 y]2,579 = 4.8 y (3.2 y) [0.0 y, 10.1 y]HD344 = 2.5 y (3.5 y) [0.0 y, 14.1 y]887 = 3.8 y (3.7 y) [0.0 y, 10.1 y]PD229 = 2.0 y (2.9 y) [0.0 y, 12.9 y]149 = 3.5 y (4.1 y) [0.3 y, 9.6 y]Tx107 = 3.2 y (2.7 y) [0.1 y, 12.1 y]94 = 4.8 y (4.5 y) [0.7 y, 9.7 y]HDDead185 = 2.0 y (3.2 y) [0.0 y, 11.8 y]398 = 1.5 y (2.5 y) [0.0 y, 9.1 y]PDDead107 = 2.3 y (3.2 y) [0.0 y, 11.7 y]47 = 2.1 y (2.3 y) [0.0 y, 8.5 y] Table 7.5: Event times for the two populations presented as Number of Events = Median (Inter-Quartile Range) [Min, Max] 7.3.2 Development Table 7.6 shows the full results from the Three-State Models, the results for the Two-State and Five-State Models can be seen in [Supplementary Material]. Older patients are predicted to be likely to transition to RRT. Increased rates of decline of eGFR were associated with the transition from CKD to RRT. VarCKD -&gt; DeadCKD -&gt; RRTRRT -&gt; Dead(Age-60)0.161 ( -0.051, 0.374)-0.041 ( -0.051, -0.031)0.063 ( 0.050, 0.076)(Age-60)^2-0.000 ( -0.002, 0.000)-0.000 ( -0.000, -0.000)log(Age)-5.725 ( -17.969, 6.518)eGFR-0.013 ( -0.019, -0.006)-0.095 ( -0.108, -0.082)0.011 ( -0.001, 0.025)eGFR Rate0.055 ( -0.021, 0.131)-0.056 ( -0.363, 0.250)log(eGFR Rate)0.042 ( -0.125, 0.210)0.227 ( -0.770, 1.225)Gender : Female-0.235 ( -0.371, -0.099)-0.277 ( -0.455, -0.099)SmokingStatus : Former_3Y-0.212 ( -0.879, 0.453)-0.133 ( -0.757, 0.490)-0.282 ( -1.082, 0.518)SmokingStatus : Non_Smoker-0.198 ( -0.345, -0.051)-0.162 ( -0.364, 0.039)-0.294 ( -0.598, 0.009)SmokingStatus : Smoker0.356 ( 0.160, 0.551)0.175 ( -0.076, 0.428)0.387 ( 0.068, 0.706)SBP-0.001 ( -0.004, 0.002)0.005 ( -0.000, 0.011)DBP0.006 ( 0.000, 0.013)0.006 ( -0.001, 0.015)BMIPrimary Renal : Familial / hereditary nephropathies-0.424 ( -0.854, 0.006)1.029 ( 0.720, 1.338)-0.562 ( -1.084, -0.040)Primary Renal : Glomerular disease-0.394 ( -0.635, -0.154)-0.165 ( -0.465, 0.134)-0.488 ( -0.883, -0.094)Primary Renal : Miscellaneous renal disorders-0.263 ( -0.505, -0.021)-0.649 ( -1.143, -0.155)0.033 ( -0.553, 0.620)Primary Renal : Tubulointerstitial disease-0.463 ( -0.741, -0.184)-0.265 ( -0.577, 0.046)-0.310 ( -0.803, 0.181)Albumin-0.044 ( -0.064, -0.024)-0.032 ( -0.059, -0.004)-0.044 ( -0.079, -0.009)Calcium0.280 ( -0.192, 0.752)-0.515 ( -1.207, 0.177)Haemoglobin-0.013 ( -0.017, -0.008)-0.005 ( -0.012, 0.001)-0.005 ( -0.014, 0.003)Phosphate0.511 ( 0.132, 0.890)0.869 ( -0.059, 1.799)uPCR0.125 ( -0.318, 0.569)0.700 ( 0.112, 1.288)-0.108 ( -0.736, 0.519)uPCR Rate-0.019 ( -0.045, 0.005)0.036 ( -0.062, 0.136)log(uPCR Rate)0.218 ( -0.310, 0.747)-0.198 ( -0.534, 0.137)DM0.122 ( -0.011, 0.255)0.141 ( -0.074, 0.358)0.200 ( -0.096, 0.496)CCF-0.394 ( -0.535, -0.253)-0.299 ( -0.597, -0.002)MI-0.246 ( -0.397, -0.094)0.234 ( -0.061, 0.530)0.186 ( -0.199, 0.572)IHD0.102 ( -0.041, 0.245)-0.077 ( -0.334, 0.179)-0.097 ( -0.424, 0.228)PVD-0.248 ( -0.394, -0.103)-0.168 ( -0.405, 0.068)-0.183 ( -0.492, 0.126)CVA-0.070 ( -0.252, 0.111)-0.168 ( -0.577, 0.240)COPD-0.289 ( -0.433, -0.145)LD-0.169 ( -0.578, 0.239)-0.316 ( -0.731, 0.097)-0.270 ( -0.858, 0.318)ST-0.274 ( -0.431, -0.117)-0.181 ( -0.516, 0.153)-0.278 ( -0.611, 0.055)HT0.274 ( -0.176, 0.726)-0.416 ( -1.104, 0.271) Table 7.6: Hazard Ratios for the Three-State Model Female patients are predicted to be more likely to remain in the CKD state than Males, or to remain in the RRT state once there. Smokers were predicted as more likely than Non-/Former Smokers to undergo any transition, apart from CKD to Tx. Blood results had associations with all transitions in some way, and disease etiology were strongly associated with the transitions giving a wide range of predictions. 7.3.3 Example The example patients seen in Table 7.1 were passed through our Three-State prediction model and the results for all time-points are shown in figure 7.2. Both patients are predicted to drop out of the CKD state at about the same rate, and their predicted probability of death after RRT is also quite close. However, the major difference in their results is the predictions in regards to RRT and the Dead (without RRT) state. Figure 7.2: Results from applying the Three-State Model to the Example Patients as a time plot Figure 7.3: Results from applying the Three-State Model to the Example Patients at discrete time-points 7.3.4 Validation 7.3.5 Calculator The calculator is currently available here: https://michael-barrowman.shinyapps.io/MSCPM_for_CKD_Patients/. 7.4 Discussion We have used data provided by SKS to develop a Multi-State Clinical Prediction Model and then validated this model within the SKS and SERPR datasets. Within our Models, the cause of a patient’s renal disease had the widest effect on patient outcomes meaning that outcomes are highly dependent on ERA-EDTA classification of the diagnosis. Most groupings resulted in a lowered hazard of death and an increased hazard of RRT compared to the baseline of Systemic diseases. [Brief discussion of validation results] [Comparison to previous work] The application of a Multi-state clinical prediction model to this field is novel and gives a powerful tool for providing individualised predictions of multiple outcomes at a wide range of time points. The general inclusion criteria for the development dataset, and the wide range of patient ages and measurements allows for the model to be applied to a broad spectrum of patients. Although the inclusion criteria for SKS were broad, the demographics of the local area resulted in homogeneity of ethnicity, which may create a limitation to the applicability of our model. The Renal Department at SRFT is a tertiary care facility for CKD sufferers and is well renowned for its capabilities of care meaning that it is likely to attract less-healthy patients from a wider catchment area, making the cohort of patients in the development population in worse condition than the general population of CKD patients. There were also high levels of missingness in the eGFR and uPCR rates of changes would also produce a bias, due to these measures likely being missing not at random. The derivation of the validation dataset ensured that all patients had an eGFR Rate measurement; this was done to avoid data missing not at random (only negative or missing data would be available as patient’s eGFR dropped to less than 60), however deriving data in this way could itself induce a survivor bias in the start date used for patients. In the Five-State Model, We omitted the analysis of the Tx to Dead state due to the anticipated low number of events within the SKS dataset. The lowest number of events for a transition was therefore PD to Dead, which had only 107. Altogether, we considered 26 covariates (with 4 categorical covariates) and so this equates to 36 predictor parameters and an events per predictor parameter (EPP) of 2.97. This is below the recommendations of Riley et al [108], whose calculations produce a requirement of 4.54 EPP. This requirement was also not satisfied by the CKD to PD transition (EPP = 6.36,required = 10.2) or the CKD to Tx transition (EPP = 2.97, required = 17.6). Fortunately, this limitation is confined to the Five-State Model. We have assumed a proportional hazards relationship between the predictors and probability of survival, which is considered by some to be a strong assumption to make, however we acknowledge this limitation, and the authors believe that it is mitigated by the flexibility that the assumption permits. In addition to the general PH assumption, the R-P model requires the assumption that the log cumulative hazard function follows a cubic spline, (however this is a much weaker assumption [95]), which is modelled as part of the regression. We did not assess the viability of these models as it was believed this assumption to make our results more understandable. [More limitations] Despite this, we believe that the advantages of our models far outweigh these shortcomings. Compared to the raw internal validation, the model performance during the external validation was worse for all metrics. However, once adjusted for optimism, the results were much more cohesive which implies that the model is highly transportable to a new population without much alterations being required. Due to the differences in the healthcare systems of England and Scotland, it can be appreciated that despite the populations being similar, their care would be different enough to emphasise a larger difference between our populations than that shown in our (relatively homogeneous) populations. Although not directly assessing causality in regards to state-transitions, our Three-State model can be used by clinicians to either expedite or delay transition of a patient onto RRT, if it is believed that this would be beneficial. Alternatively, the Five-State Model can be interpreted to provide information regarding which treatment might be benficial for a patient. Our paper has clearly demonstrated the accuracy of such a model. However, further research would be needed to establish the effectiveness and efficacy of its use in clinical practice [109] by comparing it to standard care and establishing whether the use of our model improves patient outcomes. All three models produced for this work performed well in terms of accuracy, calibration and discrimination when applied internally and externally. This shows directly that the models are suitable for use in populations similar to both our development and our validation datasets. It can also be concluded that the models can be transported and applied to any population with a similar healthcare system to the UK. "],
["chap-mscpm.html", "Chapter 8 mscpm Package", " Chapter 8 mscpm Package Here is where the Vignette for the \\{mscpm\\} package will go, when I get to coding/writing it all up. It may be easier to write a bit of code to extract the vignette from the package folder (or write something to push it from there to here) "],
["references-1.html", "References", " References "],
["chap-conclusion.html", "Chapter 9 Conclusion", " Chapter 9 Conclusion Here is where my concluding section will go. The final word. The end. "]
]
