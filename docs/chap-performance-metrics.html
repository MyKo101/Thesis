<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models | Multi-State Clinical Prediction Models in Renal Replacement Therapy</title>
  <meta name="description" content="Chapter 4 Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models | Multi-State Clinical Prediction Models in Renal Replacement Therapy" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models | Multi-State Clinical Prediction Models in Renal Replacement Therapy" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models | Multi-State Clinical Prediction Models in Renal Replacement Therapy" />
  
  
  

<meta name="author" content="Michael Andrew Barrowman" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-IPCW-logistic.html"/>
<link rel="next" href="chap-dev-paper.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="chap-lit-report.html"><a href="chap-lit-report.html"><i class="fa fa-check"></i><b>1</b> Literature Report</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="chap-lit-report.html"><a href="chap-lit-report.html#clinical-prediction-models"><i class="fa fa-check"></i><b>1.2</b> Clinical Prediction Models</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="chap-lit-report.html"><a href="chap-lit-report.html#fundamental-prognosis-research"><i class="fa fa-check"></i><b>1.2.1</b> Fundamental Prognosis Research</a></li>
<li class="chapter" data-level="1.2.2" data-path="chap-lit-report.html"><a href="chap-lit-report.html#prognostic-factor-research"><i class="fa fa-check"></i><b>1.2.2</b> Prognostic Factor Research</a></li>
<li class="chapter" data-level="1.2.3" data-path="chap-lit-report.html"><a href="chap-lit-report.html#prognostic-model-research"><i class="fa fa-check"></i><b>1.2.3</b> Prognostic Model Research</a></li>
<li class="chapter" data-level="1.2.4" data-path="chap-lit-report.html"><a href="chap-lit-report.html#stratified-medicine"><i class="fa fa-check"></i><b>1.2.4</b> Stratified Medicine</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chap-lit-report.html"><a href="chap-lit-report.html#competing-risks-multi-state-models"><i class="fa fa-check"></i><b>1.3</b> Competing Risks &amp; Multi-State Models</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="chap-lit-report.html"><a href="chap-lit-report.html#traditional-survival-analysis"><i class="fa fa-check"></i><b>1.3.1</b> Traditional Survival Analysis</a></li>
<li class="chapter" data-level="1.3.2" data-path="chap-lit-report.html"><a href="chap-lit-report.html#competing-risks"><i class="fa fa-check"></i><b>1.3.2</b> Competing Risks</a></li>
<li class="chapter" data-level="1.3.3" data-path="chap-lit-report.html"><a href="chap-lit-report.html#multi-state-models"><i class="fa fa-check"></i><b>1.3.3</b> Multi-State Models</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chap-lit-report.html"><a href="chap-lit-report.html#chronic-kidney-disease"><i class="fa fa-check"></i><b>1.4</b> Chronic Kidney Disease</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="chap-lit-report.html"><a href="chap-lit-report.html#clinical-prediction-models-1"><i class="fa fa-check"></i><b>1.4.1</b> Clinical Prediction Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html"><i class="fa fa-check"></i><b>2</b> How unmeasured confounding in a competing risks setting can affect treatment effect estimates in observational studies</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a>
<ul>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#background"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#methods"><i class="fa fa-check"></i>Methods</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results"><i class="fa fa-check"></i>Results</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#conclusion"><i class="fa fa-check"></i>Conclusion</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#supplementary-material"><i class="fa fa-check"></i>Supplementary Material</a></li>
</ul></li>
<li class="chapter" data-level="2.1" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#background-1"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#methods-1"><i class="fa fa-check"></i><b>2.2</b> Methods</a></li>
<li class="chapter" data-level="2.3" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results-1"><i class="fa fa-check"></i><b>2.3</b> Results</a></li>
<li class="chapter" data-level="2.4" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#discussion"><i class="fa fa-check"></i><b>2.4</b> Discussion</a></li>
<li class="chapter" data-level="2.5" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#conclusion-1"><i class="fa fa-check"></i><b>2.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html"><i class="fa fa-check"></i><b>3</b> Inverse Probability Weighting Adjustment of the Logistic Regression Calibration-in-the-Large</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#methods"><i class="fa fa-check"></i>Methods</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results"><i class="fa fa-check"></i>Results</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#discussion"><i class="fa fa-check"></i>Discussion</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#supplementary-material"><i class="fa fa-check"></i>Supplementary Material</a></li>
</ul></li>
<li class="chapter" data-level="3.1" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#methods-1"><i class="fa fa-check"></i><b>3.2</b> Methods</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#aims"><i class="fa fa-check"></i><b>3.2.1</b> Aims</a></li>
<li class="chapter" data-level="3.2.2" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#data-generating-method"><i class="fa fa-check"></i><b>3.2.2</b> Data Generating Method</a></li>
<li class="chapter" data-level="3.2.3" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#prediction-models"><i class="fa fa-check"></i><b>3.2.3</b> Prediction Models</a></li>
<li class="chapter" data-level="3.2.4" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#the-ipcw"><i class="fa fa-check"></i><b>3.2.4</b> The IPCW</a></li>
<li class="chapter" data-level="3.2.5" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#calibration-measurements"><i class="fa fa-check"></i><b>3.2.5</b> Calibration Measurements</a></li>
<li class="chapter" data-level="3.2.6" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#estimands"><i class="fa fa-check"></i><b>3.2.6</b> Estimands</a></li>
<li class="chapter" data-level="3.2.7" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#performance-measures"><i class="fa fa-check"></i><b>3.2.7</b> Performance Measures</a></li>
<li class="chapter" data-level="3.2.8" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#software"><i class="fa fa-check"></i><b>3.2.8</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results-1"><i class="fa fa-check"></i><b>3.3</b> Results</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#no-correlation"><i class="fa fa-check"></i><b>3.3.1</b> No correlation</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#positive-correlation"><i class="fa fa-check"></i><b>3.3.2</b> Positive correlation</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#negative-correlation"><i class="fa fa-check"></i><b>3.3.3</b> Negative correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap-IPCW-logistic.html"><a href="chap-IPCW-logistic.html#discussion-1"><i class="fa fa-check"></i><b>3.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html"><i class="fa fa-check"></i><b>4</b> Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#motivating-data-set"><i class="fa fa-check"></i><b>4.2</b> Motivating Data Set</a></li>
<li class="chapter" data-level="4.3" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#current-approaches"><i class="fa fa-check"></i><b>4.3</b> Current Approaches</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#baseline-models"><i class="fa fa-check"></i><b>4.3.1</b> Baseline Models</a></li>
<li class="chapter" data-level="4.3.2" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#notation"><i class="fa fa-check"></i><b>4.3.2</b> Notation</a></li>
<li class="chapter" data-level="4.3.3" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#patient-weighting"><i class="fa fa-check"></i><b>4.3.3</b> Patient Weighting</a></li>
<li class="chapter" data-level="4.3.4" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#accuracy---brier-score"><i class="fa fa-check"></i><b>4.3.4</b> Accuracy - Brier Score</a></li>
<li class="chapter" data-level="4.3.5" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#discrimination---c-statistic"><i class="fa fa-check"></i><b>4.3.5</b> Discrimination - c-statistic</a></li>
<li class="chapter" data-level="4.3.6" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#calibration---intercept-and-slope"><i class="fa fa-check"></i><b>4.3.6</b> Calibration - Intercept and Slope</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#extension-to-multi-state-models"><i class="fa fa-check"></i><b>4.4</b> Extension to Multi-State Models</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#trivial-extensions"><i class="fa fa-check"></i><b>4.4.1</b> Trivial Extensions</a></li>
<li class="chapter" data-level="4.4.2" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#accuracy---multiple-outcome-brier-score"><i class="fa fa-check"></i><b>4.4.2</b> Accuracy - Multiple Outcome Brier Score</a></li>
<li class="chapter" data-level="4.4.3" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#discrimination---polytomous-discriminatory-index"><i class="fa fa-check"></i><b>4.4.3</b> Discrimination - Polytomous Discriminatory Index</a></li>
<li class="chapter" data-level="4.4.4" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#calibration---multinomial-intercept-matched-and-unmatched-slopes"><i class="fa fa-check"></i><b>4.4.4</b> Calibration - Multinomial Intercept, Matched and Unmatched Slopes</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#application-to-real-world-data"><i class="fa fa-check"></i><b>4.5</b> Application to Real-World Data</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#accuracy"><i class="fa fa-check"></i><b>4.5.1</b> Accuracy</a></li>
<li class="chapter" data-level="4.5.2" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#discrimination"><i class="fa fa-check"></i><b>4.5.2</b> Discrimination</a></li>
<li class="chapter" data-level="4.5.3" data-path="chap-performance-metrics.html"><a href="chap-performance-metrics.html#calibration"><i class="fa fa-check"></i><b>4.5.3</b> Calibration</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#discussion"><i class="fa fa-check"></i><b>4.6</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html"><i class="fa fa-check"></i><b>5</b> Development and External Validation of a Multi-State Clinical Prediction Model for Chronic Kidney Disease Patients Progressing onto Renal Replacement Therapy and Death</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a>
<ul>
<li class="chapter" data-level="" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#objectives"><i class="fa fa-check"></i>Objectives</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#background"><i class="fa fa-check"></i>Background</a></li>
<li class="chapter" data-level="" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#objective"><i class="fa fa-check"></i>Objective</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#methods"><i class="fa fa-check"></i>Methods</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results"><i class="fa fa-check"></i>Results</a></li>
<li class="chapter" data-level="" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#discusssion"><i class="fa fa-check"></i>Discusssion</a></li>
<li class="chapter" data-level="" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#supplementary-material"><i class="fa fa-check"></i>Supplementary Material</a></li>
</ul></li>
<li class="chapter" data-level="5.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#methods-1"><i class="fa fa-check"></i><b>5.2</b> Methods</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#data-sources"><i class="fa fa-check"></i><b>5.2.1</b> Data Sources</a></li>
<li class="chapter" data-level="5.2.2" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#model-design"><i class="fa fa-check"></i><b>5.2.2</b> Model Design</a></li>
<li class="chapter" data-level="5.2.3" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#validation"><i class="fa fa-check"></i><b>5.2.3</b> Validation</a></li>
<li class="chapter" data-level="5.2.4" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#example"><i class="fa fa-check"></i><b>5.2.4</b> Example</a></li>
<li class="chapter" data-level="5.2.5" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#calculator"><i class="fa fa-check"></i><b>5.2.5</b> Calculator</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results-1"><i class="fa fa-check"></i><b>5.3</b> Results</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#data-sources-1"><i class="fa fa-check"></i><b>5.3.1</b> Data Sources</a></li>
<li class="chapter" data-level="5.3.2" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#development"><i class="fa fa-check"></i><b>5.3.2</b> Development</a></li>
<li class="chapter" data-level="5.3.3" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#validation-1"><i class="fa fa-check"></i><b>5.3.3</b> Validation</a></li>
<li class="chapter" data-level="5.3.4" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#example-1"><i class="fa fa-check"></i><b>5.3.4</b> Example</a></li>
<li class="chapter" data-level="5.3.5" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#calculator-1"><i class="fa fa-check"></i><b>5.3.5</b> Calculator</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#discussion"><i class="fa fa-check"></i><b>5.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-conclusion.html"><a href="chap-conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="chap-Conf-CR-supp.html"><a href="chap-Conf-CR-supp.html"><i class="fa fa-check"></i><b>A</b> How unmeasured confounding in a competing risks setting can affect treatment effect estimates in observational studies - Supplementary Material</a>
<ul>
<li class="chapter" data-level="A.1" data-path="chap-Conf-CR-supp.html"><a href="chap-Conf-CR-supp.html#simulation-details"><i class="fa fa-check"></i><b>A.1</b> Simulation Details</a></li>
<li class="chapter" data-level="A.2" data-path="chap-Conf-CR-supp.html"><a href="chap-Conf-CR-supp.html#mathematics-of-subdistribution-hazards"><i class="fa fa-check"></i><b>A.2</b> Mathematics of Subdistribution Hazards</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="chap-IPCW-logistic-supp.html"><a href="chap-IPCW-logistic-supp.html"><i class="fa fa-check"></i><b>B</b> Inverse Probability Weighting Adjustment of the Logistic Regression Calibration-in-the-Large - Supplementary Material</a>
<ul>
<li class="chapter" data-level="B.1" data-path="chap-IPCW-logistic-supp.html"><a href="chap-IPCW-logistic-supp.html#calibration-slope"><i class="fa fa-check"></i><b>B.1</b> Calibration Slope</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#results"><i class="fa fa-check"></i><b>B.1.1</b> Results</a></li>
<li class="chapter" data-level="B.1.2" data-path="chap-Conf-CR.html"><a href="chap-Conf-CR.html#discussion"><i class="fa fa-check"></i><b>B.1.2</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="chap-dev-paper-supp.html"><a href="chap-dev-paper-supp.html"><i class="fa fa-check"></i><b>C</b> Development and External Validation of a Multi-State Clinical Prediction Model for Chronic Kidney Disease Patients Progressing onto Renal Replacement Therapy and Death - Supplementary Material</a>
<ul>
<li class="chapter" data-level="C.1" data-path="chap-dev-paper-supp.html"><a href="chap-dev-paper-supp.html#statistical-analysis"><i class="fa fa-check"></i><b>C.1</b> Statistical Analysis</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#development"><i class="fa fa-check"></i><b>C.1.1</b> Development</a></li>
<li class="chapter" data-level="C.1.2" data-path="chap-dev-paper.html"><a href="chap-dev-paper.html#validation"><i class="fa fa-check"></i><b>C.1.2</b> Validation</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="chap-dev-paper-supp.html"><a href="chap-dev-paper-supp.html#model-results"><i class="fa fa-check"></i><b>C.2</b> Model Results</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="chap-dev-paper-supp.html"><a href="chap-dev-paper-supp.html#two-state-model"><i class="fa fa-check"></i><b>C.2.1</b> Two State Model</a></li>
<li class="chapter" data-level="C.2.2" data-path="chap-dev-paper-supp.html"><a href="chap-dev-paper-supp.html#three-state-model"><i class="fa fa-check"></i><b>C.2.2</b> Three State Model</a></li>
<li class="chapter" data-level="C.2.3" data-path="chap-dev-paper-supp.html"><a href="chap-dev-paper-supp.html#five-state-model"><i class="fa fa-check"></i><b>C.2.3</b> Five State Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multi-State Clinical Prediction Models in Renal Replacement Therapy</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap-performance-metrics" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Prediction Model Performance Metrics for the Validation of Multi-State Clinical Prediction Models</h1>
<p><em>MA Barrowman, GP Martin, N Peek, M Lambie, M Sperrin</em></p>
<p>Last updated: 16 Jun</p>
<p>Download as individual paper draft: <a href="Chapters/ind_05-Performance_Metrics.pdf">pdf</a>, <a href="Chapters/ind_05-Performance_Metrics.tex">tex</a>, <a href="Chapters/ind_05-Performance_Metrics.docx">word</a></p>
<div id="introduction" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>Clinical Prediction Models (CPMs) provide individualised risk of a patient’s outcome (cite), based on that patient’s predictors. These predictions will usually be in the form of a risk score or probability. However, using traditional modelling techniques, these CPMs will only predict a single outcome. Multi-State Clinical Prediction Models (MS-CPMs) combine the multi-state modelling framework to the prognostic field to provide predictions for multiple outcomes in a single model.</p>
<p>Once a CPM has been developed, it is important to assess how well the model actually performs (cite). This process is called Model Validation and involves comparing the predictions produced by the model to the actual outcomes experienced by patients (cite). It is expected that the development of a CPM will be accompanied by the validation of the model on the same dataset it was developed in (internal validation), using either bootstrapping or cross-validation to account for optimism in the developed model (cite). Models can also be validated on a novel dataset (external validation), which is used to assess the generalisability and transportability of the model (cite).</p>
<p>During validation, there are different aspects of model performance that we can assess and these are measured using specific metrics. For example, to assess the overall Accuracy of a model, we may use the Brier Score (cite) or to analyse how well a model discriminates between patients, we could use the c-statistic (cite). The current metrics that are commonly used have been designed and extended to work in a variety of model development frameworks. However, these extensions are limited to either a single outcome (as in traditionally developed models) or do not adequately account for the censoring of patients (as commonly occurs in longitudinal data).
This paper aims to provide use-able extensions to current performance metrics to be used when validating MS-CPMs. It is essential that these extensions are directly comparable with current metrics (to allow for quicker adoption), that they are collapsible to the current metrics and that they adjust for the bias induced by the censoring of patients.</p>
<p>Currently, the most common way to validate an MS-CPMs is by applying traditional methods to compare across two states at a given time and then aggregating the results in an arbitrary manner [cite something]. Other methodologists have extended existing metrics to multinomial outcomes [cite van Calster], which do not contain a time-based component; to simple competing risks scenarios [cite CR c-statistic], which do not contain transient states; or to [… insert third relevant example]. Spitoni et al [cite Spitoni 2018]] developed methods to apply the Brier Score (or any proper score functions) to a multi-state setting and so a simplified and specific version of their work is described in this paper.</p>
<p>It is the hope of the authors that this work will increase the uptake of multi-state models and the sub-field of MS-CPMs will grow appropriately.</p>
</div>
<div id="motivating-data-set" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Motivating Data Set</h2>
<p>[<strong>Table One for The Glasgow Data</strong>]</p>
<p>Throughout this paper we will use a model developed in Chronic Kidney Disease (CKD) patients to assess their progression onto Renal Replacement Therapy (RRT) and/or Death [cite Dev/Valid Paper]. The model was developed using data from the Salford Kidney Study (SKS) and then applied to an external dataset derived from the West of Scotland (see Table 2) [1]. The original model predicts the probability that a patient has begun RRT and/or died after their first recorded eGFR below 60 ml/min/1.73m2, by any time in the future (reliable up to 10 years). For the purposes of this paper, we will take a “snapshot” of the predictions at the 5 year time point.
The Three-State model used in our example is designed as an Illness-Death Model [2], this is one of the simplest MSM designs and has the key advantage over a traditional model that they can predict whether a patient is in or has visited the transient state before reaching the absorbing state (i.e. patient who became ill before dying or who started RRT before dying) (see figure 1).</p>
<p>[<strong>Figure of the MSM</strong>]</p>
<p>[<strong>Describe Glasgow Data</strong>]</p>
</div>
<div id="current-approaches" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Current Approaches</h2>
<p>Here we describe three commonly used performance metrics for assessing the performance of a traditional survival clinical prediction model. These metrics assess the Accuracy, Discrimination and Calibration of the models being validated. Accuracy is an overall measurement of how well the model predicts the outcomes in the patients. Discrimination assesses how well the model discerns between patients; in a two-state model this is a comparison of patients with and without the outcome, and should assign a higher value to those that experience the outcome. Calibration is the agreement between the observed outcomes and the predicted risks across the full risk-range.</p>
<p>We are applying cross-sectional metrics at a set time point within the setting of a longitudinal model and so we need to account for the censoring of patients and therefore, each uncensored patient at a given time t will be weighted as per the Inverse Probability of Censoring Weighting (IPCW) [3]. This allows the uncensored patient population to be representative of the entire patient population.</p>
<div id="baseline-models" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Baseline Models</h3>
<p>To assess the performance of a model, we must compare the values produced by the performance metrics to those of two baseline models; a random or non-informative model and a perfect model.</p>
<p>A Non-Informative (NI-)model assigns the same probability to all patients to be in any state regardless of covariates and is akin to using the average prevalence in the entire population to define your model. For example, in a Two-State model with an event that occurs in 10% of patients, all patients are predicted to have a 10% chance of having the event. For many metrics, models can be compared to an NI-model to assess whether the model is in fact “better than random”.</p>
<p>A Perfect (P-)model is one which successfully assigns a 100% probability to all patients, and the predictions are correct; this is the ideal case and is therefore the standard that most models aim for.</p>
<p>It may also be the case that a model performs worse than a non-informative one, however we will not consider these in detail here as they are considered to be without worth in terms of predictive ability without a well-informed adjustment.</p>
<p>The metrics produced by these baseline models will often depend on the prevalence of each state and/or the number of states. These values can be used as comparators to provide contextual information regarding the strength of model performance. These baselines metrics for the NI-model and the P-model will be referred to as the NI-level and P-level for the metric.</p>
</div>
<div id="notation" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Notation</h3>
<p>Throughout this paper, we will use consistent notation which is shown here for reference and to avoid repetition in definitions. The common notations are defined below:</p>
<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Notation
</th>
<th style="text-align:left;">
Meaning
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(N(t)\)</span> or <span class="math inline">\(N\)</span>
</td>
<td style="text-align:left;">
Number of (non-censored) patients in a population at time <span class="math inline">\(t\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(K\)</span>
</td>
<td style="text-align:left;">
Number of states predicte by the model
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(P_i^k(t)\)</span> or <span class="math inline">\(P_i^k\)</span>
</td>
<td style="text-align:left;">
Predicted probability of whether patient <span class="math inline">\(i\)</span> was in state <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(P_i^{!k}(t)\)</span> or <span class="math inline">\(P_i^{!k}\)</span>
</td>
<td style="text-align:left;">
Predicted probability of whether patient <span class="math inline">\(i\)</span> was not in state <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>, i.e. <span class="math inline">\(P_i^k + P_i^{!k} = 1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(P_i(t)\)</span> or <span class="math inline">\(P_i\)</span>
</td>
<td style="text-align:left;">
If <span class="math inline">\(K \neq 2\)</span>, vector of predicted probabilities for patient <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>, <span class="math inline">\(P_i = (P_i^1,P_i^2,...,P_i^K)\)</span>
If <span class="math inline">\(K=2\)</span>, then <span class="math inline">\(P_i = P_i^2\)</span> (i.e. predicted probability of the second state at time <span class="math inline">\(t\)</span>)
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(P^k(t)\)</span> or <span class="math inline">\(P^k\)</span>
</td>
<td style="text-align:left;">
The vector of the predicted probabilities of being in state <span class="math inline">\(k\)</span> for the whole population at time <span class="math inline">\(t\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(P(t)\)</span> or <span class="math inline">\(P\)</span>
</td>
<td style="text-align:left;">
If <span class="math inline">\(K \neq 2\)</span>, a <span class="math inline">\(N \times K\)</span> matrix of predicted probabilities for each state &amp; individual at time <span class="math inline">\(t\)</span>
If <span class="math inline">\(K=2\)</span>, a vector of the predicted probabilities of being in state 2 for the whole population at time <span class="math inline">\(t\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(O_i^k(t)\)</span> or <span class="math inline">\(O_i^k\)</span>
</td>
<td style="text-align:left;">
Binary indicator for whether patient <span class="math inline">\(i\)</span> was in state <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(O_i^{!k}(t)\)</span> or <span class="math inline">\(O_i^{!k}\)</span>
</td>
<td style="text-align:left;">
Binary indicator for whether patient <span class="math inline">\(i\)</span> was not in state <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>, i.e <span class="math inline">\(O_i^k + O_i^{!k} = 1\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(O_i(t)\)</span> or <span class="math inline">\(O_i\)</span>
</td>
<td style="text-align:left;">
If <span class="math inline">\(K \neq 2\)</span>, vector of outcomes for patient <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>, <span class="math inline">\(O_i = (O_i^1,O_i^2,...,O_i^K)\)</span>
If <span class="math inline">\(K=2\)</span>, then <span class="math inline">\(O_i = O_i^2\)</span> (i.e. observation of patient in the second state at time <span class="math inline">\(t\)</span>)
</td>
</tr>
</tbody>
</table>
<p>[<strong>Notation Table to be finished</strong>]</p>
</div>
<div id="patient-weighting" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Patient Weighting</h3>
<p>At a given time after the index date, some patients in our validation data set will be censored and so our performance metrics must adjust for this. Therefore, all patients will be subject to IPCW, which applies a higher weighting to patients who are more likely to be censored. This process is assumed to be independent of the Multi-State process, given a patient’s covariates <span class="citation">[<a href="#ref-spitoni_prediction_2018" role="doc-biblioref">66</a>]</span>.</p>
<p>To calculate this weight, first we need to estimate an individual patient’s probability of not being censored at the current time point, <span class="math inline">\(G(t|Z)\)</span>, where <span class="math inline">\(Z\)</span> is the patient’s covariate characteristics and <span class="math inline">\(t\)</span> is the current time point. This is done in our validation cohort using a Cox regression which provides estimated hazard ratios for each of the covariates <span class="math inline">\(\hat{\beta}\)</span> taking the time of censoring as the event-of-interest. Absolute predictions are then calculated using the Breslow estimate of the cumulative baseline hazard function, <span class="math inline">\(\hat{\Lambda}_0\)</span>. The estimate, <span class="math inline">\(\hat{G}\)</span>, is then given by
<span class="math display">\[
\hat{G}(t|Z) = \exp\left(-e^{\beta Z}\hat{\Lambda}_0(t)\right)
\]</span>
For a given patient, <span class="math inline">\(i\)</span>, with a maximum observed time of <span class="math inline">\(T_i\)</span>, we will define <span class="math inline">\(\delta_i = 0\)</span> if the patient was censored and <span class="math inline">\(\delta_i=1\)</span> if the patient moved to an absorbing state (e.g. died) and <span class="math inline">\(z_i\)</span> to be that patient’s set of covariates.</p>
<p>We can therefore define the IPCW for patient <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span> to be:</p>
<p><span class="math display">\[
\omega_i(t) = \frac{I(T_i \le t_i,\delta_i=1)}{\hat{G}(T_i|z_i)} + \frac{I(T_i &gt; t_i)}{\hat{G}(t_i|Z_i)}
\]</span></p>
<p>By applying this weighted to all patients included at each time point under analysis, we can be confident that our measurements are robust to right-censored data, subject to the assumptions made in their definition.</p>
<p>The metrics defined below (including those traditionally defined elsewhere) have been corrected for the effect of censoring by applying the IPCW, <span class="math inline">\(\omega_i(t)\)</span> to each patient as a multiplicative weight.</p>
</div>
<div id="accuracy---brier-score" class="section level3" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Accuracy - Brier Score</h3>
<p>For these metrics, we will be taking the measurements of the models at specific time point of <span class="math inline">\(t=5\)</span> years, and so we simplify notation by removing the references to time given above, for example <span class="math inline">\(\omega_i = \omega_i(5\;\textrm{years})\)</span>.</p>
<p>the Brier Score is used to assess the overall accuracy of predictions, it assigns a score to each observation dependent on the predicted probability and the outcome. It then averages these scores across the entire population. The Brier Score, adjusted for IPCW, for a single outcome model for a single patient is given by:
<span class="math display">\[
\textrm{BS}_i = \omega_i\left(P_i - O_i\right)^2
\]</span></p>
<p>And for the entire population, we take the weighted average given by the following <span class="citation">[<a href="#ref-brier_verification_1950-1" role="doc-biblioref">78</a>]</span>
<span class="math display">\[
\textrm{BS} = \frac{1}{N_\omega}\sum_{i=1}^N\textrm{BS}_i = \frac{1}{N_\omega}\sum_{i=1}^N\omega_i\left(P_i - O_i\right)^2
\]</span></p>
<p>A lower Brier score implies a more accurate model (since the Predictions and the Observations will be closer to one another). The P-level of the BS measure is 0 and the NI-level is <span class="math inline">\(Q(1-Q)\)</span>.</p>
<p>In order to standardise the Brier Score, we can rescale it by dividing by the NI-level and subtracting it from 1 to give the adjusted Brier Score (aBS):</p>
<p><span class="math display">\[
\textrm{aBS} = 1-\frac{BS}{Q(1-Q)}
\]</span>
The aBS brings the NI-level to 0 and the P-level to 1 and so a higher value for the aBS implies a model accurate model. One thing to note is that it is possible to get negative values for the aBS if a model performs worse than a non-informative model; however in practice this model would essentially be unusable as it is (although still useful if predictions were reversed).</p>
<p>We can use the values of <span class="math inline">\(\textrm{BS}_i\)</span> to calculate a standard deviation and thus build a confidence interval surrounding our overall BS estimate. This population-based BS confidence interval can be converted into a confidence interval for the aBS using the above formula. We will also use bootstrapping to construct a confidence interval around our estimate to compare the two methods of CI-building.</p>
</div>
<div id="discrimination---c-statistic" class="section level3" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Discrimination - c-statistic</h3>
<p>The c-statistic <span class="citation"><span class="citeproc-not-found" data-reference-id="austin_interpreting_2012"><strong>???</strong></span></span> is the most common method to assess the disciminative ability of a prediction model. In a traditional model, this cna be interpreted as the probability that two patients, chosen at random from the two outcome groups, will be correctly discriminated. Here, correct discrimination means that the patient who had the event was predicted to have a high probability of having the event than the patient who did not have the event.</p>
<p><span class="math display">\[
c = \textrm{Prob}\left(P_i &lt; P_j \;|\; O_i = 0 \;\&amp;\; O_j = 1\right)
\]</span></p>
<p>This can be estimated empirically by averaging over all pairs of patients where one is selected from each state:</p>
<p><span class="math display">\[
\hat{c} = \frac{1}{N_1N_2}\sum_{i \in A_1}\sum_{j \in A_2} \omega_i\omega_jC_2(P_i,P_j)
\]</span>
where</p>
<p><span class="math display">\[
C_2(a,b) = \begin{cases} 1 &amp; a &lt; b\\0 &amp; a &gt; b\\\frac{1}{2}&amp; a = b \end{cases}
\]</span></p>
<p>In practice, it will be very rare for two predicted probabilities to be exactly equal, but this case is needed to account for the NI-model and produce the NI-level of 0.5, we also have a P-level of 1 regardless of the prevalence of the two states.</p>
<p>Since the occurence of equal predicted probabilities is rare, the vast majority of the values for the <span class="math inline">\(C_2(a,b)\)</span> will be either 0 or 1. This means we can model the distribution of <span class="math inline">\(C_2\)</span> as a Bernoulli distribution and use this modelling assumption to construct a population-based confidence interval accordingly by taking our standard error as:
<span class="math display">\[
\sqrt{\frac{\hat{c}(1-\hat{c})}{N_1N_2}}
\]</span>
As with the aBS, this population-based confidence interval will be compared to one constructed via bootstrapping.</p>
</div>
<div id="calibration---intercept-and-slope" class="section level3" number="4.3.6">
<h3><span class="header-section-number">4.3.6</span> Calibration - Intercept and Slope</h3>
<p>In a traditional model, the Calibration Intercept is a measure of Calibration-in-the-Large, or overall calibration across the entire population <span class="citation">[<a href="#ref-altman_prognosis_2009" role="doc-biblioref">10</a>]</span>. Calibration slope indictes how well the model predicts across different prediction values. These metrics can be measured using logistic regression on the probability of the outcome using the logit of the prediction as the predictor in the regression:</p>
<p><span class="math display">\[
\textrm{E}\left[\textrm{logit}\left(O\right)\right] = \alpha + \beta\textrm{logit}\left(P\right)
\]</span>
The estimates of these coefficients, <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are found using a weighted binomial logistic regression, with weights <span class="math inline">\(\omega_i\)</span>. The intercept, <span class="math inline">\(\hat{\alpha}\)</span>, can provide a measure of any systemic over- or under-prediction of the outcome within the model. The slope, <span class="math inline">\(\hat{\beta}\)</span>, provides a measure of how well the model performs across the population, rather than simply an average of the population (as <span class="math inline">\(\hat{alpha}\)</span> is). It is advised that the intercept is calculated on its own first using <span class="math inline">\(\textrm{logit}(P)\)</span> as an offset (without a predictor, i.e. fixing <span class="math inline">\(\beta = 1\)</span>) and then the slope is calculated using <span class="math inline">\(\hat{\alpha}\)</span> as an offset <span class="citation">[<a href="#ref-riley_prognosis_2019" role="doc-biblioref">6</a>]</span>; however, for simplicity we have chosen to model them both together.</p>
<p>As the predicted values of an NI-Model would be the same for all patients, a directly calculated NI-model would not converge, however the limit of such a model (as the individual predictions tend to equality) would give NI-levels for the Intercept equal to prevalence (<span class="math inline">\(Q\)</span>) and slope equal to 0 (since every subgroup has the same predicted value). For a P-model, the Intercept would be 0 and the slope would be 1.</p>
<p>Most software which can produce these kinds of logistic regression models will have functionality to calculate confidence intervals built-in and so we will use these measures as out population-based confidence interval to compare to the one found via bootstrapping.</p>
<p>These metrics, intercept and slope, are usually described with an interpretation depending on the fit and whether the P-level (0 and 1, respectively) is within the confidence interval and, if not, which direction the miscalibration lies. If the calibration intercept is considered to be above or below the P-Level, then it indicates that the model is systemically under- or over-predicting the results, respectively. Similarly, a calibration slope that is below or above the P-Level is interpreted to mean that the model had predictions that were too extreme or too moderate across the prediction spectrum <span class="citation">[<a href="#ref-steyerberg_towards_2014" role="doc-biblioref">79</a>]</span>.</p>
</div>
</div>
<div id="extension-to-multi-state-models" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Extension to Multi-State Models</h2>
<div id="trivial-extensions" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Trivial Extensions</h3>
<p>As well as the extension methods described in this paper, each of the traditional performance metrics described above can be applied to a MS-CPM with trivial extension. These require the predictions and outcomes to be reduced toa model with only two states which allows the traditional performance metrics to be directly applied.</p>
<p>The first method, One-Vs-All, is based on whether a patient is in each state or not at a given time. For each state, we take the current state as the outcome state and collapse all other states into a single “not-” state. For example, when analysing the CKD state, we collapse RRT and Death into a single “not-CKD” state. This gives us a metric for each state in the model.</p>
<p>The second method, Pair-wise, compares across pairs of states by ignoring predictions unrelated to them at a given time. For each pair of states, we exclude patients not in one of the two states and normalise the two predicted probabilities so that they sum to 1. for example, when assessing CKD vs RRT, we exclude all patients in the Death state, take our outcome states as RRT and divide the predicted probability of being in RRT by the predicted probability of being in either CKD or RRT (i.e. probability of being in RRT given that they are in either RRT or CKD). This gives us a metric for each pair of states in the model.</p>
<p>The third method, Transition-wise, compares patients undergoing a specific transition. We take the subset of patients who were eligible for a transition and classify those who underwent the transition as being in the outcome state and compare them to those that didn’t undergo the transition (by the given time). In our example, whne looking at the RRT to Death transition, we would take the subset of all patients who underwent the CKD to RRT transition (i.e. those eligible for the RRT to Death transition) and compare those who transitioned to Death with those who remained in the RRT state.</p>
<p>Note that the subset of patients in the second and third methods are ot always equivalent. When analysing RRT to Death or RRT vs Death, the patients in the RRT state are the same, but the patients in the Death state are different (RRT vs Death includes those that went directly from CKD to Death). The predicted probabilities are similarly different.</p>
<p>When applying each of the trivial extension methods above, we would be provided with a set of metric values, e.g. in the One-Vs-All methods, we would have a value for each state in the MS-CPM. there are then two ways to summarise this information, either through a direct average of the results or through a weighted average. The weights for the Pair-wise and Transition-wise extensions would be the inverse of the size of the population being considered, for the One-Vs-All, it would be the inverse of the size of the state being compared.</p>
</div>
<div id="accuracy---multiple-outcome-brier-score" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Accuracy - Multiple Outcome Brier Score</h3>
<p>Brier’s original definition of the Brier Score <span class="citation">[<a href="#ref-brier_verification_1950-1" role="doc-biblioref">78</a>]</span> permits multiple outcomes and for an individual can be calculated as:</p>
<p><span class="math display">\[
\textrm{BS}_{i,K} = \omega_i\left(\sum_{k=1}^K\left(P_i^k - O_i^k\right)^2\right)
\]</span>
We then take an average to find the overall <span class="math inline">\(\textrm{BS}\)</span>:</p>
<p><span class="math display">\[
\textrm{BS}_K=\frac{1}{N_\omega}\sum_{i=1}^N\textrm{BS}_{i,K}=\frac{1}{N_\omega}\sum_{i=1}^N\sum_{k=1}^K\omega_i\left(P_i^k - O_i^k\right)^2
\]</span>
The formula for the traditional Brier Score is actually a simplified version of the original Brier Score defined here. Similarly, a lower score implies a more accurate model. If the two Brier Score measures are applied to a Two-State Model, then the multi-state BS above is twice that of the traditional BS, (<span class="math inline">\(\textrm{BS}_K=2\textrm{BS}\)</span>), this is because the traditional metric looks at only the outcome state, but the extended method sums over both states.</p>
<p>For this metric, the P-level is 0 and, similar to the traditional metric, the NI-Level is <span class="math inline">\(\sum_{k=1}^kQ_k(1-Q_k)\)</span>; because of this, we would need to apply an adjustment similar to the traditional Brier Score:</p>
<p><span class="math display">\[
\textrm{aBS} = 1-\frac{\textrm{BS}_K}{\sum_{k=1}^kQ_k(1-Q_k)}
\]</span>
Note that due to the relationship between <span class="math inline">\(\textrm{BS}\)</span> and <span class="math inline">\(\textrm{BS}_2\)</span>, the doubling that occurs cancels out between the numerator and denominator and so this adjustment works on the same scale as the previously defined <span class="math inline">\(\textrm{aBS}\)</span> (and thus is given the same name).</p>
<p>As with the traditional <span class="math inline">\(\textrm{BS}\)</span> metric, each patient will have their own <span class="math inline">\(\textrm{BS}_K\)</span> measurement and so we can find the population-based confidence interval for the <span class="math inline">\(\textrm{BS}_K\)</span> by using the standard deviation of these values. This can once again be converted into a confidence interval for the <span class="math inline">\(\textrm{aBS}\)</span> and compared to the one found via bootstrapping.</p>
</div>
<div id="discrimination---polytomous-discriminatory-index" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Discrimination - Polytomous Discriminatory Index</h3>
<p>Intuitively, the extension of the c-statistic would be the probability that <span class="math inline">\(K\)</span> patients, chosen randomly from each of the outcome groups, will be correctly discriminated. In this case, what it is to be correctly discriminated needs to be defined. The Polytomous Discriminitory Index (PDI) provides a definition for this discrimination <span class="citation"><span class="citeproc-not-found" data-reference-id="clster_extending_2012"><strong>???</strong></span></span>. We define a <span class="math inline">\(K\)</span>-uple of patients as an ordered set of <span class="math inline">\(K\)</span> patients where one patients is from each of the outcomes. A <span class="math inline">\(K\)</span>-tuple of patients s well discriminated for a state <span class="math inline">\(k\)</span> if the patient in state <span class="math inline">\(k\)</span> was predicted to have the highest probability of being in state <span class="math inline">\(k\)</span> compared to the others in the <span class="math inline">\(K\)</span>-tuple. If we let patients <span class="math inline">\(i_j\)</span> be a patient in state <span class="math inline">\(j\)</span>, then the PDI for state <span class="math inline">\(k\)</span> in that <span class="math inline">\(K\)</span>-tuple can be given as:</p>
<p><span class="math display">\[
C_K^k(i_1,i_2,...,i_k,...,i_K)=\begin{cases}1 &amp; P_{i_k}^k &gt; \max\left(P_{i_j}\;:\;j\neq k\right)\\0 &amp; P_{i_k}^k &lt; \max\left(P_{i_j}\;:\;j\neq k\right)\\\frac{1}{m} &amp; P_{i_k}^k  =\max\left(P_{i_j}\;:\;j\neq k\right),\;m=\left|\left\{j\;:\;P_{i_j}^k=P_{i_k}^k\right\}\right|\\\end{cases}
\]</span></p>
<p>This definition also includes the caveat that if there are ties for the maximum predicted probability by assigning <span class="math inline">\(\sfrac{1}{m}\)</span> when that occurs, where <span class="math inline">\(m\)</span> is the number of patient (including <span class="math inline">\(i_k\)</span> tying for highest probability).</p>
<p>For a <span class="math inline">\(K\)</span>-tuple of patients, we also define their combined IPCW as the product of their individual IPCWs. This allows us to define a PDI for a <span class="math inline">\(K\)</span>-tuple in a given state.</p>
<p><span class="math display">\[
\textrm{PDI}_K^k(i_1,i_2,...,i_K)=\left(\prod_{j=1}^K\omega_{i_j}\right)C_K^k(i_1,i_2,...,i_K)
\]</span>
This allows us to define average weighted PDI for a <span class="math inline">\(K\)</span>-tuple of patients as:</p>
<p><span class="math display">\[
\textrm{PDI}_K(i_1,i_2,...,i_K)=\frac{1}{K}\sum_{k=1}^K\textrm{PDI}_K^k(i_1,i_2,...,i_K)
\]</span></p>
<p>Or, we can summarise by finding the average PDI for a given state across the whole population:</p>
<p><span class="math display">\[
\textrm{PDI}_K^k = \left(\frac{1}{\prod_{k=1}^KN_k}\right)\sum_{i_1 \in A_1}\sum_{i_2 \in A_2}...\sum_{i_K \in A_K}\textrm{PDI}_K^k(i_1,i_2,...,i_K)
\]</span></p>
<p>These averages can be averaged again to get an overall measure of PDI:</p>
<p><span class="math display">\[
\begin{align*}
\textrm{PDI}_K&amp;=\frac{1}{K}\sum_{k=1}^K\textrm{PDI}_K^k \\&amp;= \left(\frac{1}{\prod_{k=1}^KN_k}\right)\sum_{i_1 \in A_1}\sum_{i_2 \in A_2}...\sum_{i_K \in A_K}\textrm{PDI}_K(i_1,i_2,...,i_K)
\end{align*}
\]</span>
Similar to the c-statistic, the P-model would score a PDI of 1, however the NI-model would achieve a PDI of <span class="math inline">\(\sfrac{1}{K}\)</span>. Therefore, we need to adjust this PDI to correct the scaling to be that of he common c-statistic:</p>
<p><span class="math display">\[
c = \left(\textrm{PDI}_K\right)^{log_K(2)}
\]</span></p>
<p>Since this new measure is on the same scale as the c-statistic, we can just refer to it as such.</p>
<p>As with the c-statistic, values of the <span class="math inline">\(C_k^K\)</span> which are neither 0 nor 1 will be rare and so we can once again model this as a Bernoulli distribution. For calculation of confidence intervals, we take our <span class="math inline">\(n\)</span> as the number of possible <span class="math inline">\(K\)</span>-tuples. We can then compare this population-based confidence interval with a bootstrapped estimate.</p>
<div id="computational-limitations" class="section level4" number="4.4.3.1">
<h4><span class="header-section-number">4.4.3.1</span> Computational Limitations</h4>
<p>One major drawback of the PDI is that for large datasets and/or with many states, it can be computationally intensive. Therefore, an estimated PDI can be found by taking a sample of the <span class="math inline">\(K\)</span>-tuples. To ensure robustness against censoring, each <span class="math inline">\(K\)</span>-tuple should be drawn into th sample with probability inverse to the IPCW of that sample, where the IPCW of a <span class="math inline">\(K\)</span>-tuple is calculated above as the probability of its elements. This is equivalent to drawing patients from each outcome with probability <span class="math inline">\(\sfrac{\omega_j}{N_\omega}\)</span>. In this case, the calculations of the PDI remain similar, but each patient would be reset with a <span class="math inline">\(\omega_j=1\)</span> (as the weighting has already been applied during sampling).</p>
</div>
</div>
<div id="calibration---multinomial-intercept-matched-and-unmatched-slopes" class="section level3" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Calibration - Multinomial Intercept, Matched and Unmatched Slopes</h3>
<p>Since the traditional calibration metrics described above use a binomial logistic regression, it seems logical that the multi-dimensional extension for a multi-state models uses a multinomial logistic regression to provide parallel interpretation <span class="citation">[<a href="#ref-hoorde_assessing_2014" role="doc-biblioref">80</a>]</span>. Unlike the other measures, we must choose a state to be our base-state, <span class="math inline">\(k=1\)</span>, this is usually the most populous initial state; however this choice is arbitrary and clinical reasoning may lead to a more logical choice. We then estimate the following series of regressions for all <span class="math inline">\(k&gt;1\)</span>:</p>
<p><span class="math display">\[
\textrm{E}\left[\textrm{log}\left(\frac{O^k}{O^1}\right)\right] = \alpha_k+\beta_{2,k}\textrm{log}\left(\frac{P^2}{P^1}\right) + ... +\beta_{K,k}\textrm{log}\left(\frac{P^K}{P^1}\right)
\]</span>
Once again, using <span class="math inline">\(\omega_i\)</span> as weights for each patient during the regression process. This process estimates the <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to provide a <span class="math inline">\((K-1)\)</span> length vector of intercept terms, <span class="math inline">\(\hat{\alpha} = \left\{\hat{\alpha_2},\hat{\alpha_3},...,\hat{\alpha_K}\right\}\)</span> and a <span class="math inline">\((K-1)\times(K-1)\)</span> dimension matrix of slope terms, <span class="math inline">\(\hat{\beta}\)</span> with subscripts running from 2 to <span class="math inline">\(K\)</span> in both dimensions.</p>
<p>The baseline models produce values similar to those found in the traditional calibration intercept and slope metrics, but directly extended to a multi-dimensional space. The P-Level for the Intercept would therefore be the zero-vector of length <span class="math inline">\((K-1)\)</span> and the Slope would be the Identity matrix for <span class="math inline">\((K-1)\)</span> dimensions. The NI-Level for the Intercept would be the prevalence (without the first state), <span class="math inline">\(\left\{Q_2,Q_3,...,Q_K\right\}\)</span>, and the Slope would be the zero-matrix for <span class="math inline">\((K-1)\)</span> dimensions.</p>
<p>Once again, software packages that can produce multinomial logistic regression <span class="citation"><span class="citeproc-not-found" data-reference-id="ripley_nnet_2016"><strong>???</strong></span></span> can also automatically produce confidence intervals surrounding these estimates, which can be arranged as a CI-vector and CI-matrix and can be compared to bootstrapped estimates.</p>
<p>as discussed earlier, traditional calibration measures are often associated with an interpretation depending on whether the model over- or under-predicts or has predictions that are too extreme or too moderate. Because of this, the multinomial extensions of these metrics cannot be aggregated to a single value (as with the other performance metric extensions), since doing so would lose a lot of information, instead we simplify in such a way to allow for a similar interpretation (or set of interpretations).</p>
<p>We count the number of values in the <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> that are considered statistically greater than, less than or close to the P-Level (based on the confidence interval). We further stratify by the <span class="math inline">\(\hat{\beta}\)</span> counts by whether the counts are along the main diagonal or not. This gives an interpretation of how many states are over- or under-predicted (based on <span class="math inline">\(\hat{\alpha}\)</span>) and whether or not predictions are too extreme or too moderate (based on the main diagonal of <span class="math inline">\(\hat{\beta}\)</span>).</p>
<p>A new interpretation can be applied to those counts supplied by the off-diagnoal <span class="math inline">\(\hat{\beta}\)</span> values. This is how strong or weak the assumption of independence of irrelevant alternatives holds in our model <span class="citation">[<a href="#ref-arrow_social_2012" role="doc-biblioref">81</a>]</span>. If we have a lot of 0 values in the off-diagonal, <span class="math inline">\(\hat{\beta}\)</span>, then the assumption is strong, whereas a lot of (statistically) non-zero values implies that the assumption is weak. If this assumption is weak, it means that there is an interactive effect between states and that if one were removed from the model, the relative probabilities of the remaining states would be effected. Whereas, if the assumption holds strongly, the removal of one state would imply that the other states are more stable and their relative probabilities would remain the same.</p>
</div>
</div>
<div id="application-to-real-world-data" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Application to Real-World Data</h2>
<div id="accuracy" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Accuracy</h3>
<p>Due to the prevelance of the different states in our population, a traditionally developed model would have to achieve a Brier Score better than [<strong>xxxx</strong>] to be better than a non-informative model, which translates to an adjsted Brier Score of [<strong>xxxx</strong>]. Our Three-State Model would therefore also have to score better than [<strong>xxxx</strong>].</p>
<p>Amongst the Pair-wise, One-Vs-All and Transition-wise based Brier Scores, the best score is the RRT One-Vs-All score of 0.04 and the</p>
</div>
<div id="discrimination" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Discrimination</h3>
</div>
<div id="calibration" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Calibration</h3>
</div>
</div>
<div id="discussion" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Discussion</h2>
<p>In this paper, we have extended the current methods of model validation to a Multi-State framework, applied them to a previously developed MS-CPM and then directly compared them to traditional methods.</p>
<p>some of the methods demonstrated here were developed by others in categorical outcome data <span class="citation"><span class="citeproc-not-found" data-reference-id="calster_extending_2012"><strong>???</strong></span>, [<a href="#ref-brier_verification_1950-1" role="doc-biblioref">78</a>]</span>;however, we are the first to apply them to a Multi-State scenario and, by providing suitable adjustments to the original work, we have provided versions of these metrics which can be comparable regardless of the number of states involved. Clearly, a model with more states provides more information to the prognosticator. Before this work, it was previously un-assessable whether the additional information came at a cost to model performance. For example, we can assess whether adding more states to a model has a negative effect on the predictive ability of a model.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-riley_prognosis_2019">
<p>[6] R. D. Riley, D. van der Windt, P. Croft, and K. G. M. Moons, <em>Prognosis Research in Healthcare: Concepts, Methods, and Impact</em>, First. Oxford University Press, 2019.</p>
</div>
<div id="ref-altman_prognosis_2009">
<p>[10] D. G. Altman, Y. Vergouwe, P. Royston, and K. G. M. Moons, “Prognosis and prognostic research: Validating a prognostic model,” <em>BMJ</em>, vol. 338, p. b605, May 2009, doi: <a href="https://doi.org/10.1136/bmj.b605">10.1136/bmj.b605</a>.</p>
</div>
<div id="ref-spitoni_prediction_2018">
<p>[66] C. Spitoni, V. Lammens, and H. Putter, “Prediction errors for state occupation and transition probabilities in multi-state models,” <em>Biometrical Journal. Biometrische Zeitschrift</em>, vol. 60, no. 1, pp. 34–48, Jan. 2018, doi: <a href="https://doi.org/10.1002/bimj.201600191">10.1002/bimj.201600191</a>.</p>
</div>
<div id="ref-brier_verification_1950-1">
<p>[78] G. W. Brier, “Verification of forecasts expressed in terms of probability,” <em>Monthly Weather Review</em>, vol. 78, no. 1, pp. 1–3, Jan. 1950, doi: <a href="https://doi.org/10.1175/1520-0493(1950)078%3C0001:VOFEIT%3E2.0.CO;2">10.1175/1520-0493(1950)078&lt;0001:VOFEIT&gt;2.0.CO;2</a>.</p>
</div>
<div id="ref-steyerberg_towards_2014">
<p>[79] E. W. Steyerberg and Y. Vergouwe, “Towards better clinical prediction models: Seven steps for development and an ABCD for validation,” <em>European Heart Journal</em>, vol. 35, no. 29, pp. 1925–1931, Aug. 2014, doi: <a href="https://doi.org/10.1093/eurheartj/ehu207">10.1093/eurheartj/ehu207</a>.</p>
</div>
<div id="ref-hoorde_assessing_2014">
<p>[80] K. V. Hoorde, Y. Vergouwe, D. Timmerman, S. V. Huffel, E. W. Steyerberg, and B. V. Calster, “Assessing calibration of multinomial risk prediction models,” <em>Statistics in Medicine</em>, vol. 33, no. 15, pp. 2585–2596, Jul. 2014, doi: <a href="https://doi.org/10.1002/sim.6114">10.1002/sim.6114</a>.</p>
</div>
<div id="ref-arrow_social_2012">
<p>[81] K. J. Arrow, <em>Social Choice and Individual Values: Third Edition</em>. Yale University Press, 2012.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-IPCW-logistic.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-dev-paper.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["thesis.pdf", "PDF"]],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
