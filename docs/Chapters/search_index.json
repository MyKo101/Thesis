[
["introduction.html", "Literature Report 1 Introduction", " Literature Report MA Barrowman 1 Introduction Formatting with: Format Choice: html and Format Choice2: Single. lorem ipsum blah blah blah "],
["clinical-prediction-models.html", "2 Clinical Prediction Models 2.1 Fundamental Prognosis Research 2.2 Prognostic Factor Research 2.3 Prognostic Model Research 2.4 Stratified Medicine 2.5 Examples", " 2 Clinical Prediction Models The idea of prognosis dates back to ancient Greece with the work of Hippocrates [1] and is derived from the Greek for “know before” meaning to forecast the future. Within the sphere of healthcare, it is definde as the risk of future health outcomes in patients, particularly patients with a certain disease or health condition. Prognosis allows clinicians to provide patients with a prediction of how their disease will progress and is uaully given as a probability of having an event in a prespecified number of years. For example, QRISK3 [2] provides a probability that a patient will have a heart attack or stroke in the next 10 years. Prognostic research encompasses any work which enhances the field of prognosis, whether through methodological advancements, field-specific prognostic modelling or educational material designed to improve general knowledge of prognosis. Prognostic models come under the wider umbrella of predictive models which also includes diagnostic models; because of this most of the keys points in the field or prognostic modeling can be applied to diagnostic models with little to no change. Prognosis allows clinicians to evaluate the natural history of a patient (i.e. the course of a patient’s future without any intervention) in order to establish the ffect of screening for asymptomatic diseases (such as with mammograms[3]). Prognosis research can be used to develop new definitions of diseases, whether a redefinition of an existing disease (such as the extension to th definition of myocardial infarction to include non-fatal events [4]) or a previously unknown subtype of a disease (such as Brugada syndrome as a type of cardiovascular disease[5]) In general, prognosis research can be broken down into four main categories, with three subcategories [6]: Type I: Fundamental prognosis research [3] Type II: Prognostic factor research [7] Type III: Prognostic model research [8] Model development [9] Model validation [10] Model impact evaluation [11] Type IV: Stratified Medicine [12] For a particular outcome, prognostic research will usually progress through these types, beginning with papers designed to evaluate overall prognosis within a whole population and then focusing in on more specificity and granularity towards individualised, causal predictions. The model development and validation will usually occur in the same paper [13], [14]. studies into all three of the subcategories of prognostic model research should be completed before a model is used in clinical practice [15], although this does not always occur [8]. External validation is considered by some to be more important than the actual deviration of the model as it demonstrates generalisability of the model [16], whereas a model on it’s own may be highly susceptible to overfitting [Cite: Something]. 2.1 Fundamental Prognosis Research [What is it? Old definition is incorrect, so will need to write this fresh] 2.2 Prognostic Factor Research The aim of prognostic factor research (Type II) is to discover which factors are associated with disease progression. This allows for the general attribution of relationships between predictors and clinical outcomes. Predictive factor research can give researchers and clinicians an idea of which patient factors are important when assessing a disease. It is vital to the development of clinical predictive models as without an idea of what covariates can affect an outcome, we cannot figure out which variables will affect the outcome. For example, [xxxx] demonstrated that [xxxx] is correlated with [xxxx], which subsequently used as a covariate in the development of the [xxxx] model. Note the use of the word correlate here as prognostic relationships do not have to be causal ones [Cite: Something]. These factors may indeed represent an underlying causal pathway, but this is not a requirement and it would require aetiological methods to discern whether it were causal or not. For example, when predicting [xxxx], we can demonstrate that [xxxx] is a prognostic factor, [however since the arrow of causation is [xxxx]] [OR] [however since [xxxx] causes both [xxxx] and [xxxx]], the relationship is prognostic, but not causal. [Previously used Apgar score here, reference 40] Counter to the idea that prognostic factors aren’t always causal, they are always confounding factors for the event they predict. Thue prognostic factors should be taken into account when planning clinical trials as if they are wildly misbalanced across the arms (or not accounted for in some other manner), they can cause biases in the results [7]. Sometimes these factors are so strong that adjusting the results of a clinical trial by the factor can affect, or even reverse the interpretation of the results [17]. If a prognostic factor is causal, then by directly affecting the factor, it can causally affect the outcome. By discovering new prognostic factors, and investigating their causality, we can potentially open the door to new directions of attack for treatments. It is unfortunate, however, that Riley at al [18] found that only 35.5% of prognostic factor studies in paediatric oncology actually reported the size of the effect of the prognostic factor they reported on. This means that very little information can be drawn from these studies. It is also important that prognostic factor research papers consider and report on the implications of the factor they assess such as healthcare costs. These kinds of implications are rarely assessed, especially when compared to drugs or interventions [7]. 2.3 Prognostic Model Research Predictive factors can be combined into a predictive model, which is a much more specific measurement of the effect of a factor on an outcome [8] and they are deigned to augment the job of a clinician; and not to completely replace them [11]. Diagnostic prediction model can be used to indicate whether a patient is likely to need further testing to establish the presence of a disease [13;~moons_transparent_2015]. Prognostic prediction models can be used to decide on further treatment for that patient, whether as a member of a certain risk group, or under a stratied medicine approach [13], [14]. Outcomes being assessed in a prediction model should be directly relevant to the patient (such as mortality) or have a direct causal relationship with something that is [11]. There is a trend of researchers focusing on areas of improvement that are of less significance to the patient than it is to a physician [Cite: LR - 18]. For example, older patient’s might prefer to have an improved quality of life than an increase in life expectancy, and thus models should be developed to account for this. Creating a clinicaly useful model is not as simple as just using some availble data to develop a model, despite what a lot of researchers seem to believe [Cite: Something]. To quote Steyerberg et al [8]. \" To be useful for clinicia,s a prognostic model needs to provide validated and accurate predictions and to improve patient outcomes and cost-effectiveness of care\". This means that, although a mdel might appear to be useful, its effectiveness is only relevant to the population it was developed in. If your population is different, then the model will behave differently. Bleeker [19] developed a model to predict bacterial infections in febrile children with an unknown source. The model scored well when assessed for the predictive value in the development dataset, however it scored much worse in an external dataset implying that, though it worked well in the development population, it would be unwise to apply it to a new population. 2.3.1 Model Development The first stage of having a useful model is to develop one. Clinical predictive models can take a variety of forms, such as logistic regression, cox models or some kind of machine learning. Regardless of the specific model type being used, there are certain universal truths than should be held up during model development which will be discussed here. The size of the dataset being used is of vital importance as it can combat overfitting of the data, but so is choosing which prognostic factors to be included in the final model. This section will discuss various ideas that researchers need to account for when developing a model from any source and can be applied to any model type. By considering a multivariable approach to prediction models (as opposed to a univariable one), researchers can consider different combinations of predictive factors, usually refered to as potential predictors [Cite: LR - 2]. These can include factors where a direct relationship with the disease can be clearly seen, such as tumour size in the prediction of cancer mortality [Cite: LR - 5], or ones which could have a more general effect on overall health, such as socioeconomic and ethnicity variables [Cite: LR - 55]. By ignoring any previous assumptions about a correlation between these potential predictors and the outcome of interest, we can cast a wider net in our analysis allowing us to catch relationships that might have otherwise been lost [Cite: LR - 56]. Prediction models should take into account as many predictive factors as possible. Demographic data should also be included as these are often found to be confounding factors, variables such as ethnicity and social deprivation risk exacerbating the existing inequality between groups r LR(7)`. When developing a predictive model, the size of the dataset being used in an important consideration. A typical “rule of thumb” is to have at least 10 events for every potential predictor [Cite: LR - 57, 58], know as the Events-per-Variable (EPV). Recently, this number has been superseded by a methods to evaluate a specific required sample size [Cite: Riley, EPV work]. If there aren’t enough events to satisfy this criteria, then some potential predictors should be eliminated before any formal analysis takes place (for example using clinical knowledge) [Cite: LR - 59]. In general, it is also recommended that this development dataset contain at least 100 events (regardless of number of potential predictors) [Cite: LR - 39, 60, 61]. A systematic review by Counsell et al [Cite: LR - 62] found that out of eighty-three prognostic models for acute stroke, less than 50% of them had more than 10 EPV, and the work by Riley et al [Cite: riley EPV Work] showed that less that [Pull example from Riley EPV]. Having a low EPV can lead to overfitting of the model which is a concern associated with having a small data set. Overfitting leads to a worse prediction when the model is used on a new population which essentially makes the model useless[Cite: LR - 34]. However, just because a dataset is large does not imply that it will be a good dataset if the quality of the data is lacking [Cite: LR - 39]. Having a large amount of data can lead to predictors being considered statistically significant when in reality they only add a small amount of information to the model [Cite: LR - 39]. The size of the effect of a predictor should therefore be taken into account in the final model and, if beneficial, some predictors can be dropped at the final stage. Large datasets can be used for both development and validation if an effective subset is chosen. This subset should not be random or data driven and should be decided before data analysis is begun [Cite: LR - 39]. Randomly splitting a dataset set into a training set (for development) and a testing set (for internal validation) can result in optimistic results in the validation process in the testing set. This is due to the random nature of the splitting causing the two populations to be too exchangeable, which is similar to the logic behind the splitting of patients in a Randomised Control Trial (RCT). Splitting the population by a specific characteristic (such as geographic location or time period) can result in a better internal validation [Cite: LR - 35, 63]. Derivation of the QRISK2 Score [Cite: LR - 7] (known later as QRISK2-2008) randomly assigned two thirds of practices to the derivation dataset and the remainder to the validation dataset [Check how QRISK3 Does this]. The NPI model [Has this been mentioned?] was trained on the first 500 patients admitted to Nottingham City Hospital after the study began [Cite: LR - 5] and later validated on the next 320 patients to be admitted [Cite: LR - 64], this validation was not performed at the same time as the initial development and is thus an external validation. If a sufficient amount of data is available and it has been taken from multiple sources (practices, clinics or studies), then it should be clustered to account for heterogeneity across sources [Cite: LR - 65]. It is important that any sources of potential variability are identified (such as heterogeneity between centres) as this can have an impact on the results of any analysis [Cite: LR - 1, 39]. Heterogeneity is particularly high when using multiple countries as a source of data [Cite: LR - 66] or if a potential predictor is of a subjective nature, which leads to discrepancies between assessors [Cite: LR - 67]. Overlooking of this clustering can lead to incorrect inferences [Cite: LR - 65]. The generalisability of the sources of data should also be considered in the development of a model. For example, the inclusion and exclusion criteria of an RCT can greatly reduce generalisability if used as a data source [11]. A prediction model researcher needs to select clinically relevant potential predictors for use in the development of the model [Cite: LR - 34]. Once chosen, researchers need to be very specific about how these variables are treated. Any adjustments from the raw data should be reported in detail 2.3.2 Model Validation 2.3.3 Impact Evaluation 2.4 Stratified Medicine 2.5 Examples Competing Risks &amp; Multi-State Models "],
["competing-risks-multi-state-models.html", "3 Competing Risks &amp; Multi-State Models", " 3 Competing Risks &amp; Multi-State Models "]
]
